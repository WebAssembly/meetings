![WebAssembly logo](/images/WebAssembly.png)

## Agenda for the October 19th video call of WebAssembly's Stack Subgroup

- **Where**: zoom.us
- **When**: October 19th, 16:00-17:00 UTC (October 19th, 9am-10am Pacific Daylight Time)
- **Location**: https://zoom.us/j/91846860726?pwd=NVVNVmpvRVVFQkZTVzZ1dTFEcXgrdz09

## Participants

Francis McCabe
Luke Imhoff
Thomas Lively
Paul Schoenfelder
Zalim Bashorov
Zhi An Ng
Ross Tate
Andreas Rossberg
Daniel Hillerström
Sam Lindley
Alon Zakai
Heejin Ahn
Asumu Takikawa
Derek Schuff

## Agenda items

1. Opening, welcome and roll call
    1. Opening of the meeting
1. Find volunteers for note taking (acting chair to volunteer)
1. Adoption of the agenda
1. Discussions
   1. [Challenges in implementing Erlang processing in WASM]() (Paul Schoenfelder) [35 mins]
   1. Call for presentations [3 mins].
1. Closure

## Meeting Notes

### Adoption of the agenda

### Discussion:

Presentation by Paul Schoenfelder

Erlang has an actor model.  Everything happens in an actor.  It is its unit of concurrency.  Strongly typed, but dynamically typed.  Kinda pure functional, but does support side-effects.  Variables are immutable.  Usually do mutation by communicating with actors.

A brief overview of terminology: functions in modules, modules in applications, applications in a system.  System is what you deploy.

Actors in Erlang are built on these core primitives: spawn them, exit them, send/receive between them, and link/monitor between them.  Link/monitor allow supervision.  This is important because supervision allows to construct key like structure with root that spawns processes.  By subscribing to those failures, problems can be constrained to a subcomponent of the tree.

In Erlang when you exit and restart a process, you can feed in the same state and start from a clean slate.

[Showing slide 5]

THis is a simple module with a process.  Passing in a function capture.  The `!` is how  you send a process.  The `Pid` is a process identifier.  `self()` is a process’s own Pid that can be sent in a message.

A lower case word like `ping` is an atom, like a string.

[Pointing at `loop`] This function will be run in a new process.  The `receive` loop` is matching on messages to the process.  

Anything uppercase is a variable in Erlang (`Parent`). 

It is matching on a message and sending back to the `Parent` process.  The receive is a blocking operation.

After one 1 second (these values are in milliseconds [points at 1000]).

`throw` is more used for non-local control flow, not exception handling.  There are 3 kinds of exceptions: `exit`, `error`, and `throw`

[Slide 6]

Unit of concurrency is actor, called a process.  Process in Erlang are green threads.  Thereis no `yield` statement in Erlang. There are more processes scheduled on fewer operating system thread.  Pre-emptive scheduling.  Scheduling is done by counting reductions.  When the process exceeds a count, the process is switched.

In Erlang each process has a heap/stack and each is garbage collected independently.  No stop-the-world GC.  If the memory is fixed for a process it is more like region-based GC.

[Slide 7]

The BEAM is a register based virtual machine.  The BEAM Is the reference implementation for Erlang.  It produced BEAM bytecode ahead of time.  At runtime the VM loads the byte code and does some optimizations.

One of the key features of the BEAM, it supports hot code reloading: hot upgrades and downgrades.  It is neat how it does it, but puts limitations on optimizations.

[Slide 8]

How it ties into WASM.  Can we bring BEAM to WASM?  A lot cannot be ported directly because of implementation techniques.  Even if you can port the VM itself, the cost to support hot up/downgrade and fully dynamic functional calls, means not optimizations and have to ship all modules.

TL:  Would be ok if yu had a tool to push up the initial version of the code and if you had hotcode pushing it up later.

PS: it would probably be possible to rework the BEAM code loader to fetch those resources on demand. Right now it assumes access on  local filesystem. Don’t think there’s a reason why it couldn’t be refactored to support something different. That’s part of the rewrite that would need to happen within BEAM to support Wasm. Possible that may happen, also wonder that is attacking the wrong issue. If you can’t strip dead code, you have to load it dynamically, you don’t know what you don’t have. This also happens (depending on system), call function, runtime notices function code isn’t present, attempts to load it.

FM: elaborate on how much bytecode we’re talking about? Whether this is part of std lib or application?

PS: minimum size of installation sits around 25MB of bytecode. Not counting VM itself. One of the libraries that I maintain, written in elixir( generates BEAM bytecode), 16MB. Larger than fair number of libs, but not the biggest you can build. If you do something like strip debug info, you can reduce the size a little bit, but still quite large. Impractical for shipping to browsers. Not a problem for non-browsers.


[Slide 9]

We’re working on lumen, it’s an ahead-of-time, native compiler.  More restrictions, no hot code loading, more optimizations.  WE can also do other optimizations that hte BEAM cannot do because of having the whole program.  Currently we’re using LLVM for the code generation.

[Slide 10]

Here’s where we get into the tricky parts where supporting Erlang vs a native environment: Green/trheading, exception handing and the hooks for garbage collection.  The way that Erlang does stack allocation is different: the stack is in the process heap.  It is more different heap allocators.  When implementing GC on a native stack we need to reason about live values across multiple GC save points.  That is what we’re using LLVM on x86_64.  It is not supported for the WASM target. t hat is one of the problems

I didn’t mentin this, but Erlang does not have a concept of iteration without recursion. There is no for or while loop.  IT is implicit in the implementation that we have proper tail calls.  Currently we are able to get enough of the support we needed.

So, for green threading, we’re getting, each process gets its own native stack.  We use assembly shims to switch between processes and the scheduler.  Swap to the scheduler, then swap to next process.  We insert yield points in the generated code.  From the assmelbpy shim perspective the scheduler is not special.  The shims are saving the state into special structure and returning to the address of the last swap.  We construct hte stack of each process, so when you first jump into the process, the stack frames are setup like you called an init process like it was being called directly.  

It is reliant on being able to save and restore registers and stacks as a standalone object. 

There is no notation of saving and restoring registers in WASM.  On every other architecture we cna use the same scheduler.  For WASM we need a completely different scheduler.  Our currently blocker is we don’t have a way to do this.  The solution to this is a first-class stack.

IF I was going to reason about it.  At native stack-like object that can be suspended, passed around, and resumed.  Create a stack, suspend the stack, resume a stack, and destory one.

Some restrictions so you can’t destroy your own.  A privileged root stack.

The thing that is most important is the first class primitive.  When I was first coming up with the compiler I was looking into using continuations similar to how Scheme uses continuation.  When we come back to WASM there is no way to arbitrarily suspend and resume.  Relooper will not work for a program that is completely based on continuatons.

TL: Can yu take a question?  Have you looked into binaryen to use Asyncify.

PS: did look at the paper for that yes. Trying to remember what the blocker is there…

TL: Was there a paper for Asyncify? There is relooper which restructures control flow. Asyncify is a separate thing, code transform that allows you to resume and suspend stacks. Dumps all local stacks of Wasm VM into memory so you can resume by reloading from memory.

[Link from Alon Zakai: https://kripken.github.io/blog/wasm/2019/07/16/asyncify.html ]


PS: pretty sure i looked at that, will have to look at what the issue was there. What i was thinking of was, code transform on JS to allow it to support first class continutations. Gets tricky when bringin all these features together, not super tied to a particular architectural. Might be some approaches if all we were supporting is Wasm. Need different run time support depending on target. IDeally you want a core runtime with some target specific code, code-gen backend with target specific stuff happening. Ultimately you don’t want two wildly different backends. If you need to go down a different path, you get that problem. To  support Wasm we have to rewrite huge portions.

SL: do you have a JS backend?

PS: no JS backend. Native code via llvm. Technically emscripten will support it.

SL: it soundslike it is possible for you to have different backend..

PS: practical issue, but is every production compiler that wants to support Wasm have to do that? Doesn’t mean  that it’s critical, alot of support already goes that route. Puts a limitation on Wasm as the substrate of the future. For it to be well-rounded, it should be able to take any source language. If you already have an ecosystem in space, want to add Wasm, then becomes a practical issue.We  have some conversation about first-class stack/continutations... 

<meeting cuts off>

PS: what i was saying, those first-class stacks can allow us to compile to Wasm without re-engineering the entire compiler.

LI:  We don’t use asyncify, how does it interact with all the things Wasi want, shared-nothing. Do you have to asyncify the whole program? Does everything have to cooperate.

AZ: anything that is on the stack must have been compiled with asyncify.

LI: I’ld be concerned about that, thinking about how BEAM work son ref interp. You can call C code,doesn’t know called from C code, can’t be preempted, you can call if it  works fast enough.

AZ: if you call c code, it is guaranteed to  exit, only a problem if it pauses inside c code.

PS: how does this interact with switching between thousands of separate threads of execution. Every computation is happening in a process, many of  them. Typically 1000s, tens of 1000s.

LI: we’ve tested the runtime can  spawn 64k of  these in browser tab. Not 10 async,  10s of  1000s.  Browsers already support, can’t compile runtime to use it right now.

RT: process, and  switching. How sensitive are you to the overhead of this. If you double/triple that runtime, do you care about that? Or is it not part of the picture.

PS: definitely noticeable, but depends on what the overhead. If O(milliseconds), will be way too slow for erland system. Typical  switch time is in nanoseconds. We wouldn’t want it to  be  more  than 1 order of magnitude.

RT: sounds like, you’ll take what you can get, if  different options, 2 or 3 x slower, you wil notice that difference.

PS: yea, but if the difference is support v.s.  Not support, then we will  take  what we can  get.

LI: phoneix using websocket going to js can stream video just fine. If we cannot replace js with lumen compiled erlang, that’s annoying. We won’t want it so low that you can’t manage and av stream with it.

RT: GC root collection, shadow stack you maintain, is that reasonable?

PS: definitely one approach. Haven’t tried reworking codegen backend. Where I left off last time was trying to figure out if we can have some sort of stack map support. Done some research into adding support to LLVM. no notion of instruction pointer in Wasm, or return address, not concepts in Wasm. That’s where the stack map support is built in LLVM, without those pieces you have to come at it from a new angle. Provide something in Wasm  similar to how gc staypoint in LLVM works. Wasm if you have something similar, you can  get similar functionality. For us, important thing is to access that root, to follow it, and rewrite it. Use native stack for most things, need to do in shadow stack. It means we have to rework a bunch of codegen backend, accepted that for Wasm that’s a reality.Try to reuse as much as possible. Non-local control flow style imposes a lot of overhead. We are accepting overhead for those cases. C++ style exceptions are ero overhead when not raised. When raised they are expensive, traverse stack, multiple places cleanup. In erlang, it is a stack  of continuation pointers, you can setjmp/longjmp for those. A lot faster than average case for control flow. A lot easier for constructing a parser with deeply nested fn calls, and you want to jump out. If you throw a lot of those kinds of exceptions, overhead of C++ style exceptions become noticeable. How do we implement exceptions without continuations. I would use continuations to implement all tehse, green threads, exceptions. We are similar to scheme in a lot of ways, the kinds of things we need.

FM: one of the reasons we’re in this conversation, we are looking at extending Wasm to include stack switching of some form. Would liek to unpack a number, stack switch measured in nanoseconds?

PS: order of 50 to 200 ns. Specifically the cost of the context switch. ~50 instructions.

RT: you want EH to be similar, you would use cont to implement EH, just jump out of stack to where you want to go instead of unwind.

PS: save the point where the last handler was saved, continuation point, jump straight to there. The strategy I was looking at using was to implement continuations on top of setjmp longjmp, or some other equivalent assembly shim. Quickly realized that won’t translate to Wasm in any meaningful sense. Tried a more tradition codegen, instead of CPS IR and codegen approach, lower things in SSA, tried generating similar to what a C compiler would do. Some things become a bit awkward, worked fairly well, main problem is green threads and scheduling.

LI: on thread about unwind on EH proposal, we use LLVM invoke instructions.

PS: anything that gets added to llvm, we can take advantage of.  Nothing can be added to LLVM to solve our problem, it’s more a runtime thing.

FM: scheduler, you have one that manages all the process in one erlang application. Typical erlang system, you have one scheduler per core on system. In practice there is a separate pool of scheudulers for IO, natively implemented. 

FM: if you do yield, how do you know what scheduler to pas to

PS: the thread runs on a scheduler, when that process suspends, it jumping back to the scheduler, process cannot be moved to another scheduler while running. Each process cooperates with its scheduler. Scheduler can communicate to steal process from other schedulers.

FM: when running on browser, you have to integrate with browser loop. We rewritten how the core scheduler loop works for running in browsers, rather than running our own loop, we let the browser run that loop, we are taking a slice of time to check and see what we need to do, schedule a process for execution, run a bit, then yield back to browser.

LI: run from a frame, assume RAF is 60 fps. Scheduler loop invoked with RAF, made sure to not go ove r16.6ms. If every process is waiting for I/o, we also yield early.

FM: means special scheduler for browser.

LI: no, how we enter the scheduler, how the scheduler is invoked

PS: different entry point, bulk of scheduler remains

LI: scheduler has run call that runs single thing. Another wrapper, that yields. On native we yield to check signals. On browsers, we yield to RAF. Check if there is signal, or allow JS to run. Both have concepts of interrupted. We are shoving signals onto a signal catching thread, instead of signal handlers go into your normal code.

PS: boils down to being able to create stacks, suspend resume them. Most accurate  maps to how our implementation works on all arch. Open to continuations too, first class stacks might be more intuitive, also to other languages. Continuations can be implemented on top of stack-switching. From my POV, they are kinda the same thing.

AR: i was about to ask what is the diff. To me one is the implementation, the other being the concept.

LI: the one place we want to make sure of… effect handler thing where everything has to work together. In order to handle callbacks from JS for like forms, we need an async callback, right now we do JS shim, tell the scheduler go start aprocess, doesn’t immd jump into Wasm, normal RAF is where we’re doing our own loop. Async and await part that the Google team wants, to be able to handle JS api callbacks without having to write our own shins.

SL: Want to get a feel of the range of usecases you have for your particular impl, you mentioned streaming and so forth, does thtt involve a lot of context switching.

PS: not targeting specific usecases, other than being able to build software in browser using BEAM languages.

SL: particularly in browser?

PS: other Wasm platforms will be interesting, primarily in Browsers. Elixir community,  large portion of them building webapps, write frontend and backend using same language, same tooling (not isomorphic like JS). That’s where the biggest win is. No specific use-case in mind. That example is to show how sensitive we are to context switch. Typical erlang systems 1000s of processes, executing some code. THe more I/O heavy you are, the less impt context switch overhead is, more cpu bound, then it becomes noticeable.

SL: you might have though for browser application, you’re likely to be I/O heavy.

LI: whatsapp, gsm phones, on top of erlangs, 2 or 3 companies built on erlang for a/v. Probably interested to use elixir on top of Wasm. We don’t want Wasm to be a thing erlang isn’t supported on.

FM: at the end of the hour.

PS: will send all my notes, less meandering of an explanation of Erlang, and background of our implementation and issues we run into, will try and answer some questions.

### Closure
