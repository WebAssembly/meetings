![WebAssembly logo](/images/WebAssembly.png)

## Agenda for the June 2 video call of WebAssembly's SIMD Subgroup

- **Dates**: 2023-06-02
- **Times**:
    - 4pm-5pm UTC (9am-10am PDT)
- **Location**: *link on calendar invite*

### Registration

You are a new attendee, please fill out this [form](https://forms.gle/9eB2ZYaziPEcTJabA) to attend.

## Agenda items

1. Opening, welcome and roll call
    1. Opening of the meeting
    1. Introduction of attendees
1. Find volunteers for note taking
1. Adoption of the agenda
1. Proposals and discussions
    1. i16x8 laneselect
    1. flexible vectors
1. Closure

## Meeting notes

### Attendees

- Andrew Brown
- Anton Kirilov
- Deepti Gandluri
- Marat Dukhan
- Petr Penzin
- Yury Delendik
- Zhi An Ng

### i16x8 laneselect

https://github.com/WebAssembly/relaxed-simd/issues/125

ZN: 8-bit blend instruction doesn’t match semantics to 16-bit laneselect, there are a few solutions: change spec, change lowering, etc

MD: keep as performant as possible, keep it to pblendvb lowering, it requires spec text changes, change it to also be able to look at top bit of each byte

PP: the 16 bit won't look at top bit of the lane, all instructions will look at top bit

MD: initially we specify to look at any subset of bits, we narrow it down to look at each bit or top bit of lane, so allowing to also look at top bit of the byte is minor relaxation

ZN: Alex brought up that if we add this, all four laneselects can use pblendvb and technically all instructions become the same operation

PP: then you can drop the ‘sign bit of a lane’ part and just keep the ‘byte’

MD: in XNNPack we detect case of sign bit of 32 bit lane and specialize for that, an implementation that uses pblendvb won't use this opt, but will still be better than bitselect

ZN: since it is a minor change and we are not technically in phase 4 is it OK to do that?

DG: totally fine, minor spec text fix can still happen, removing instructions, adding new ones, need more oversight, still at phase 3. We discussed previously about phase 4, require more profiles discussion. Especially if we know spec has a mistake

ZN: to summarize we will make a spec change for 

DG: we will want to post on the issue about our discussion before doing work

PP: if anyone wants to take advantage of all 3 options, it will require detecting whether implementation use pblendvb for everything, or have different instructions, you have to detect

ZN: yes, will have to check i32x4 implementation if you want to use i32x4 laneselect to check f32x4 sign

PP: if you define operation to be byte wise then you can prepend different comparison with different lane sizes, that will result in the semantics. You can define it to be byte wise.

ZN: this would add an instruction to the lowering

PP: the effect would be negligible, one extra instruction for SSE and two more for AVX

AB: the point of the proposal is to try to get performance back, think going for the most performant option is the way to go

ZN: what about removing i16x8 laneselect

MD: will be weird to ask user to use i8x16 laneselect, if result is mask, you can use the the right lane type for the laneselect

DG: we can remove operation at this phase, any of the options here are fine from spec / phase perspective

AK: on ARM, all options can implement this, which is single instruction compare against 0, my preference is efficiency on all

MD: question for firefox, when will relaxed simd become stable?

YD: no plans, at the moment waiting for official phase 4


### flexible vectors

Slides: [(pdf)](presentations/2023-06-02-flexible-vectors.pdf)

AK: we have element wise shifts, same concerns applies, maybe some clarification there to do

PP: proposal has elementwise shifts, they are variables, not immediates

AK: what happens when indices are out of bounds? if you look at pseudocode, it's probably better to have explicit spec text, maybe pseudocode needs tweaking

PP: if more than number of lanes, you get all 0, should have a note that explicitly says that

AK: in pseudocode, index the vectors, doesn't make sense if index is > vector

PP: pseudocode does a check but needs tweaking

DG: for most bounds checking, at least in v8, it's ~15-20% performance regression across the board for scalar code. Any ideal performance you have in mind?

PP: need to look at mechanisms for OOB, how exp it is, hope you can eliminate a lot of the bounds check because they will be produced by constants, indices are constants, easily detectable. if not, the modular scenario will be better. Less about performance i can estimate, more of whether if a compiler can detect this easily

AK: on x86 no special handling needed for int divisions right? i'm thinking on aarch64, int divs never trap there, we have to check what input values are, if 0 in order to do trapping behavior that is mandated by Wasm. Div is always done unconditionally, branch is not on critical path, expect that bounds check for laning is done in a similar way. Vector op will be done unconditionally, and a check and conditional branch that is not on critical path, will have some performance impact on larger code size, but likely not that bad.

PP: check version will be more friendly, will need optimizations to ensure it doesn't kill performance. Also need to explore 128 bit chunks, if it is easier to use, then skew it towards 128 bit chunks, then get lanes using immediates.

AK: have a comment about 3rd option, taking modulo the lane count. In AArch64 we deprecated non power of 2 vector sizes

PP: it would make this option a bit better, no need to do division or do it efficiently, always a power of 2

PP: x86 also the same, multiples of 128, power of 2 * 128. only odd arch is RISC-V. but that isn't considered the main target at this point.

<missed some discussions here>

AK: on ARM you need to target a particular microarch, might be a bit too constraining, less optimal lowering, idea about having shuffle that operate on 128 bit chunks, have to look at the specific ops, but might be challenging, very generic shuffle in SVE is not designed that way.

PP: not possible to do the same on both platforms

AK: if op is not available on the spec, and user is forced to workaround, if it is as costly to workaround as having in the spec, then still make sense to have the spec

PP: highway and halide as a source of inspiration, libraries like this define all of it, some op clearly work better on x86, some work better on arm. that is similar to SIMD. we survived that

PP: need to get to the prototyping phase, a lot of things we need to test and try. Broad consensus this is the right op, is this ready for phase 2?

ZN: set of ops look complete, book a slot to present to CG?

DG: would like to understand the goals, who will be using this? what are you trying to achieve? what sort of hardware? Ideal performance scenarios? Need to scope this out to motivate implementations. For phase 2, need to build what this will look like. Digging deeper into what applications you expect to care about wide vectors in the world of GPUs. How does this look like on older devices?

PP: classes of instructions not work exploring in 128 SIMD, someone brought up crypto, we talked about compatibility between two platforms, talked to people about crypto, if only has 128 SIMD, performance will be similar.

DG: wouldn't you include AES instruction and not SIMD

PP: you will need AES to be longer than 128, it is quite close to scalar

PP: some int operations used in graphics will be affected, do we care about crypto or similar use cases

DG: from chromium, we have seen crypto requests, for AES 128, web crypto has its own problems. For people outside web crypto but want a fallback, there is Wasm fallback, but complicated for them to use. Trying to see if we want to prototype AES 128. Try to not fragment ecosystem with multiple implementation of crypto

PP: in the future we open up avenue for adding them

DG: being able to do fast crypto on web using Wasm is not ....

DG: for a lot of this, we need tooling to catch up more, we can provide tooling solutions for people that care about crypto. e.g. a request is to share the same AES header, if decrypting, might not be the critical path. They are trying to have a single implementation across all platform, an emscripten header they can share across all platforms that can target native and Wasm. Solution space is large.

AK: crypto might be tricky, some people care whether crypto ops are timing independent, another proposal for constant time, might need to coordinate with other proposal

PP: could also apply to people hand rolling theire own crypto

DG: another thing for profiles, constant time and deterministic profiles. we haven't seen activity on constant time proposal for several years, don't know if it is active.

AK: AArch64 already has constant time equivalent extension, looked at the spec before, lists instructions that take constant time, AES are in that list. would assume that if we do flexible vectors, it will look the same, need to coordinate.
