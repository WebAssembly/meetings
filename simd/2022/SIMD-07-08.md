![WebAssembly logo](/images/WebAssembly.png)

## Agenda for the July 8th video call of WebAssembly's SIMD Subgroup

-   **Dates**: 2022-07-08
-   **Times**:
    -   4pm-5pm UTC (9am-10am PDT)
-   **Location**: *link on calendar invite*

### Registration

You are a new attendee, please fill out this
[form](https://forms.gle/9eB2ZYaziPEcTJabA) to attend.

## Agenda items

1.  Opening, welcome and roll call
    1.  Opening of the meeting
    1.  Introduction of attendees
1.  Find volunteers for note taking
1.  Adoption of the agenda
1.  Proposals and discussions
    1.  Relaxed BFloat16 Dot Product (Marat Dukhan)
1.  Closure

## Meeting notes

### Attendees

- Andrew Brown
- Anton Kirilov
- Evan Nemerson
- Illya Rezvov
- Johnnie Birch
- Justin M
- Marat Dukhan
- Petr Penzin
- Richard Winterton
- Thomas Lively
- Yury Delendik
- Zhi An Ng

### Relaxed BFloat16 Dot Product (Marat Dukhan)

https://github.com/WebAssembly/relaxed-simd/issues/77

MD: newer arm and intel CPUs have bfloat 16 support, however differences in implementation, moreover ARM has differences in ARM v8.6 and ARM v9.2, up to 2x speed up on CPUs that support this.

PP: newer CPUs that support this, thats AVX 512 extension, pretty rare

MD: not common so far, but those CPUs are on the market

RW: future will have it

PP: bfloat 16 is useful exclusively in ML operation, not something for general purpose, because it lacks floating point numbers. we don't have a bfloat16 type to begin with, not sure if we should be adding something that we don't have

MD: we don't need new instructions to support bfloat16 type, it's just high 16 bits of float32, can work with it using current instructions, even without Wasm SIMD

PP: accelerators would actually manipulate it directly

MD: that's why i'm proposing this, when hardware supports this can be efficient

PP: it's not completely free right?

MD: pretty much the same way you do it in software, each 32 bit lane will have 2 packed bfloat16, extract each of them into two multiply adds

RW: how does lowering know it's a bfloat 16

MD: lowering reads them as 2 16 bit ints, pad with 0 in low bits, then uses floating point multipliers

RW: treated as 16 bit integers, how do you know bfloat 16

MD: add special instructions to process a dot product by prepadding bfloat16 with 0s in low 16 bits

ZA: why not add in the future, when more machines in the world support it

MD: don’t want to create a new proposal for this instruction

PP: some existing SIMD operations can be implemented with AVX 512, AVX 512 is so rare in client hardware that implementations like V8 don't care about it. Second point is that we have existing holes in proposals, e.g. missing extend for x86, we have 8 -> 16, 16 -> 32, but not 8 -> 32, x86 supports, cheap to emulate in ARM. We have to file a new proposal for this.

MD: the issue with extend is that ARM lowering is not optimal, not any better than current lowering. With bfloat16, emulation is expensive.

PP: doesn't negate what was said about AVX512, from V8 POV, AVX512 is not feasible.

MD: can't speak for V8, but AFAIU, AVX512 will get popular enough that V8 will care about it, or intel will backport to AVX2, like VNNI, there will be a version for consumer CPUs

RW: there will be a set of instructions that will support AVX 512, back ports

AB: having seen bfloat16 improvements in performance that people have presented, i'm in favour of this type of thing, that's a way of speeding up inference and training. less worried about current support of AVX 512, petr brought up correctly that client machines don't have it, on server side it is supported by currently shipped chips, and also in the future, and possibly supported in client. less concerned that it currently isn't widespread

ZA: Is this one bfloat16 instruction enough? Is it sufficient to speed up core computations, or would we need more instructions?

MD: this single instruction is sufficient, principalling we need 3 primitives, extension from bfloat16 to 32, easy toe mulate with Wasm SIMD, fp32 to bfloat16, not worth adding to Wasm SIMD, can truncate, in most use case, most computations in fp32, only bfloat16 for storage, we need an instruction for FMA, which is this dot product.

RW: adding this into relaxed SIMD now wouldn't inhibit us from moving this into a non-relaxed version, you can implement it otherwise.

MD: to have it as a regular instruction, has to be bitwise compatible between architectures

RW: that's what you will have to do if you want to migrate that to SIMD

MD: don't see that it's possible as a non-relaxed instruction, it would have to be emulated, not using hardware instructions

RW: if there is a 5 year window that we see it is worth emulating, then it could be

MD: yes, if we look at the spec, latest version on ARM is not x86. current hardware instructions don't provide it. on x86, instructions treat denormals as 0, regardless of global settings, on ARM, compute dot product as fused operations without intermediate rounding in 9.2, then addition with rounding.

RW: so you think going forward there will not be support

MD: will have to see, currently is not identical

AB: MD, confirming what you’ve said - when we have a bfloat 16, we need conversion to and fro 32, we will just take 16 bits, thats equivalent to rounding towards 0, and that's fine for these use cases?

MD: fine if you don't do it very often, for neural network, we do it at the boundaries

AB: this point about rounding and conversion to and from fp32, should document in the issue, in the future, we might get questions why we don't implement the conversion instructions

RW: we can add in conversion instructions in the future

MD: conversion instructions are hard to emulate without hardware support

PP: what's the difference between adding conversion ops and not adding?

MD: there are libraries doing this truncation, haven't heard issues about accuracy, when using bfloat16 you already lose a lot of accuracy. Should be addressed with IEEE fp16 conversion in the future. Think Wasm will eventually need to fill this gap. That's completely outside of relaxed SIMD. That can be done bit-exact.

ZA: anyone wants to raise a point against this instruction

PP: from engine, AVX 512 is not supported

AB: what engines

PP: V8, no AVX 512 support

YD: no plan to support AVX 512 any time soon

AB: maybe we should get an implementability comment from the main engines, is this something we actually want

ZA: have implementers comment about support for AVX 512 and ARM v8.6 with bf16 extension

AB: for ARM we have CPUID check

PP: no EVEX in V8

MD: amd is about to start flooding market with ZEN 4, which will have AVX 512 bf 16, probably consumer variants

PP: rumor is that they will come up with AVX 512, will every single consumer chip have it? it's a relatively rare instruction set

YD: not just CPUID, also fingerprinting question, better to see fallback implementation for no AVX512, to see how deterministic they are. Otherwise it will be easy to detect what CPU it is.

AB: opening the can of worms this has opened, by running a combination of the other instructions, one can figure out

YD: in this case, if we don't document fallback, every engine will choose their own implementation

PP: right now we can distinguish x86 and ARM easily, will also allow us to distinguish different kind of chips, does NEON have bfloat16

MD: on arm, can implement this in a way that doesn't expose information, slightly less efficient, need 2 native instructions instead of 1

ZA: We need comments from engine teams on viability of native lowering on the issue. Yury if you can comment there and I’ll ask Deepti to comment as well.

AK: looking at issue, about emulating with 2 native instruction, don't understand, looks like both sides use bf16

MD: can get bitwise identical result with and without bf16 extension

AK: can measure performance right?

MD: yes, that's always possible

AK: on ARM, 2 generations of client cores that support, current server has (AWS graviton 3)

MD: cpus on market that we can buy today, chip in Galaxy S22 seems to support it

AK: soc vendors disable some extensions for reasons, some SOC include necessary support, graviton 3 definitely support bf16

MD: bfloat 16 supported in US galaxy S22

AK: no physical phone to check, going by CPU core documentation

JB: quick question on fingerprinting, is it primarily on the client or server side?

PP: more on client, server side you likely already know what is the server, on client it's wild code running on your device

RW: probably steer clear of that and let security team deal with that
