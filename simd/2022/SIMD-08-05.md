![WebAssembly logo](/images/WebAssembly.png)

## Agenda for the August 5th video call of WebAssembly's SIMD Subgroup

- **Dates**: 2022-08-05
- **Times**:
    - 4pm-5pm UTC (9am-10am PDT)
- **Location**: *link on calendar invite*

### Registration

You are a new attendee, please fill out this [form](https://forms.gle/9eB2ZYaziPEcTJabA) to attend.

## Agenda items

1. Opening, welcome and roll call
    1. Opening of the meeting
    1. Introduction of attendees
1. Find volunteers for note taking
1. Adoption of the agenda
1. Proposals and discussions
    1. BFloat16 experiments (Marat Dukhan)
1. Closure

## Meeting notes

### Attendees

- Andrew Brown
- Anton Kirilov
- Deepti Gandluri
- Johnnie Birch
- Justin Michaud
- Marat Dukhan
- Petr Penzin
- Richard Winterton
- Zhi An Ng

### BFloat16 experiments (Marat Dukhan)

MD: on cpus like X2 with native FP16 support, 2x speedup v.s. fp32 on instruction level, using fp16 dot product compared to fp32 FMA, approximately 2x faster, 4x faster compared to without relaxed simd. haven't done matrix multiplication test. want to discuss whether such matrix mult benchmarks will be adequate to make a decision or not. Won't completely close the gap with native, at least on ARM, there are native isntructions which do native fp16 matrix mul. this is a simple way to get the benefit of bfloat16 with just 1 instruction

DG: was this ARM64?

MD: yes, all current CPUs which support BFloat 16, same throughput as for FP32, but BFloat16 will produce twice as many results

DG: this is if you were to use bfloat16 on arm64, diff is purely performance, any semantic difference?

MD: if we use dot product there will be semantic diff, we can use 2 instructions isntead of 2, some small speed up to avoid conversion between bfloat and f32, will need to try after complete impl

AB: any x64 tests?

MD: no access to x64 CPUs with BFloat 16 extension, Raptor Lake should have it, Alder Lake with efficiency cores disabled unofficially support it.

PP: that's turned off in BIOS now, won't be able to do that

PP: extremely rare, only supported in XEON, not usable by everyday users

MD: Wasm is not restricted to desktops

PP: only want to note this

MD: will be AMD systems, with support for BFloat 16

PP: any link for this?

MD: linked in the PR https://www.anandtech.com/show/17399/amd-ryzen-7000-announced-zen4-pcie5-ddr5-am5-coming-fall

DG: elaborate on the 2 instructions?

MD: base line, unpack odd bfloat, unpack even bfloat, do 2 fmas. We can save unpack instructions with 2 instructions, will preserver bit exact compatibility with simulation code that would use four for olden CPUs.

JM: for performance numbers, have anything more holistic? not just instruction itself, but a more representative use case?

MD: no complete matrix multiplication benchmark, expect to have it over the weekend

JM: would like to know with the 2 instructions what performance will be. If there is a risk of compat difference between 2 versions of ARM chips, that's not something we want to risk.

MD: 2 ways to lower, directly bf dot instructions in bf16 instruction, this has different semantics than simulation code (even, odd, fmas). ARM also has bfmla instructions, specifically for this use case, when we want to bit exact with emulation code with previous generation CPUs. These instructions come with cost, twice as expensive as bfdot, but are on today's CPU. if we target bit-exact compat, then it is acceptable.

JM: want to make sure we have this data point, thanks

DG: from chrome perspective, still trying to get user data of how we slice down user stats, for fingerprinting, depending on how many people are on the newer subset. We still have older users we have to support, don't want to increase fingerprinting. Engines can dynamically make that decision, depending on what exactly we want to support, how we see that performance to deterministic trade off. Risk of fingerprinting on this instruction seems high. Not opposed to include in spec, but won't see us getting the performance off the bat.

MD: would be acceptable to start with 2 instructions today, bfloat16 in armv9, and eventually all CPUs will be on armv9 and later, then we can switch to bfdot

DG: to JM, curious about how you think about including something in spec v.s. how deterministic engine can be

JM: agree with you so far, Apple cares a lot about fingerprinting, in this instance more concerned about compatibility. Haven't seen a lot of applications using relaxed simd, my impression is that this instruction is very early, most users don't have a computer that supports it, from that perspective, we should be cautious to not introduce compatibility cliffs

MD: is your concern about performance cliffs when instruction is simulated?

JM: concerned about a bug where some website works fine in Intel but doesn't work on ARM. Acceptable if there is an enormous performance benefit. If it is 2%, seems not worth the risk or extra complexity. Other parts of the spec are clearer on performance benefit on wide range of data. Curious about actual performance of this instruction.

MD: do you know if Apple M2 supports bfloat 16

JM: no idea, haven't figured it out yet

MD: Apple recently added M2 detection to cpuinfo library, probably already supported or maybe in future iterations

JM: i don't have an M2 yet

ZA: AI on Marat for 

DG: where do we stand on adding new instructions to make it more future proof? we have to spin off relaxed-simd spec because we didn't include instructions, so we don't have to spin off another new proposals.

PP: we realize that the determinism was an obstacle in some case, relaxed simd explores slightly different aspects, need to relax the spec, if we stick to original assumptions to SIMD, won't be adding any of these. flexible vectors explore something different also.

DG: say we don't add bfloat16... some things i run into is that applications run into performance bottlenecks with out of scope instructions.

PP: realistically, this is a separate aspect, so far there is an instruction for this, so far we are talking about speculation, targeting future, not yet implemented instruction

DG: started to think about this for bfloat16. Posing it as an open question.

MD: would be problematic to have an extension for just one instruction, a lot of organization to go through all the phases. An instruction would need some strong champion to push it through, if it is not in relaxed-simd, probably won't happen at all. Wasm doesn't have any runtime instruction detection, if Wasm modules have instructions not supported, engine will reject entire module, any subset of Wasm instruction extensions have to be treated as a different instruction set. Already today there are many combination, MVP, MVP+SIMD, MVP+SIMD+Threads, MVP+threads. The more instruction extensions we add, the bigger support matrix. Software designed for particular subset supported by web browsers, we risk getting into situation where developers start targeting particular browsers. Overhead and risk in adding small extensions. Hard to manage to library and application devs.

PP: super important, there is a problem with our process, need to resolve it somehow. Just adding extra things people have to buy wholesale.

JM: very reasonable, the other point is, if CPU vendor adds a bfloat16 with semantics just subtly different, that don't match what we decide to spec, could be a huge performance cliff. Not sure if the safe option is to add the instruction, or not have it. Have to look at performance numbers. Very hard to predict that. Was there any discussion of other ways to get this performance? Perhaps allowing speculative optimizations or allow instruction selections in the engine, applications can hint that they are fine with some rounding behavior for different instruction patterns.

DG: had couple of approaches, could still come in the future, something similar to clang fast math. In the tools (Thomas Lively will know more), applications can specify this, and they can generate. The autovectorizer won't use this directly. If you want fast math, can generated relaxed simd, probably. Right now the only way is to use intrinsics. We still need these SIMD instructions, especially for Wasm, there is no way to say we can reduce determinism a bit. Engines can do this at some point, but probably not the right way, each engine can have different behaviors.

PP: what happened in SIMD discussion a lot is compare different lowerings between architectures, don't think hints were really discussed. question about relaxed simd and autovectorizer, is that the plan?

DG: depends on a bunch of different things, if you enable it in autovectorizer, easy for random applications to pick it up when we won't want them. Have compiler flags to enable these instructions.

PP: if fast math enables this in autovectorizer, that's' fine. If this can only be used in intrinsics in the future, we cut out 90% of developers

MD: target here is dev who write matrix multiplications, you don't accidentally write bfloat 16 matrix mul

DG: Marat and Zhi are library authors, assuming authors know what they are writing, applications just use the libraries, we are seeing this a lot in Wasm and SIMD specifically, they are built into backends, they are mostly not going to try this new intrinsic out. It is based on how library support them, this is the case of XNNPack, Halide, Draco. We are enabling them from behind the scenes. That usage model for me is the right one.

PP: what i mean is that 90% of people who write arithmetic in their source

DG: we are not expecting them to use Wasm intrinsics, they are using C or Rust and compiling. If the autovectorizer is not using them, how are these visible to them

PP: if they write arithmetic, would they get relaxed simd instructions

DG: potentially if they use fast math, otherwise they won't

PP: don't know if it requires platform detection to work correctly, needs to be tested in toolchain

DG: engines decide what code to generate based on extension (SSE2, SSE4), they do a lot of work to ensure that same outputs

PP: for some libraries like XNNPack, they have different code based on runtime arch.

DG: toolchain person not around, we should talk about what toolchain support looks like

MD: back to original question on whether we need a fast math mode to give this ability to engines, that mode was originally in relaxed simd proposal, discussed and the conclusion is that it shouldn't be in relaxed simd, it can't guarantee that scalar computations follow semantics, first due to vectorizations, second due to cpu modes, this modes affect both scalar and simd computations, if this semantic is added to Wasm, should be higher level, not as part of relaxed-simd proposal.

PP: there was a proposal for adding different arithmetic modes in general

JM: thanks, does anyone know what the status is feature detection in Wasm, any proposals? If that area is what is holding us back, then it is worth pushing harder on that.

DG: two proposals, conditional segments, newer feature detection proposal https://github.com/webassembly/feature-detection. runs into issue of how to make this proposal generic enough to be used outside of SIMD. E.g. for threads, how to detect shared memory, worker based threads. Other thing is also libraries built on top of Wasm, there is a wasm-feature-detect JS library. Tried to make a hard push at some point to get feature detection across the board in conjuction with relaxed-simd. Given the number of issues, we split them, relaxed-simd has a better defined scope.

JM: has anyone discussed about requiring engines supporting all the semantic behaviors? do we have a mode where the engine is required to run in all of them, so application developers can't forget to test against configurations? When they have SLAs they artificially have downtime so downtime services don't depend on too high SLA.

MD: some features are hard to emulated when not supported in hardware, e.g. FMA, emulation is very slow, it would negate all the impact of SIMD. Not really possible to support all options on CPU which don't support them.

JM: i guess most web browser implementations are tiered, a lot of the SIMD instructions, as long as instructions tier up, potentially haven't a few iterations running in the slow mode might not be enough to offset. If instructions doesn't perform a big enough performance benefit to offset that performance.

MD: we guaranteed that runtime will always lowered the same. If you have FMA, it will also be FMA, or non-fused. Will break in software

JM: It would break on CPUs which developers didn't test on

MD: there are algorithms which require FMA, and algorithms that completely break if compiler fuse certain computations. There is a good chance it would break on platforms that aren't tested. We discussed an option for Wasm engine to use some non-standard behavior, not behavior that is native in this platform. At least not in V8.

DG: for fused or unfused results, the baseline compiler and optimizing compiler, we try to make sure they are emitting similar code. We don't support all of the lower end hardware, it is a decode error.

JM: my concern is that application developer will write it, and only test on intel on chrome on a new PC, then it will be broken on ARM, or browsers with different lowering, if there is anything to try to encourage developers to test on all configurations.

DG: been talking about this, platform test for web api features. Maybe you can run in a sandbox environment to test compatibility, but we could not find a feasible path to emulation.

MD: already possible to distinguish between x86 and ARM, in principle, possible to write software that is broken on one v.s. another, unlikely you can accidentally write such it.

PP: OpenIPC, demo of h264 or h265 on the web, using SIMD performance was not the same, some browsers have 2x speedup, others have 4x (not terrible though).

DG: one thing we tried in V8 is have a scalarizer in engine, got too hard, and maintaining semantics correctly, it didn't justify what we were seeing. If relaxed-simd had scalar lowering, to make it accurate will be really hard.

JM: make sense, we've had a bug report where a website that worked in Chrome but not Safari, turns out we didn't expose the number of logical processors, we always return 1, fingerprinting risk. Chrome and Firefox returns actual numbers, they have code that did something divided by processors - 1. Worry that some hardware configurations, which are hard for users to get, for example for developers in other countries. The concern is that we create a future where some platforms have a higher risk of bug, because devs don't test on them as frequently.

DG: one thing we can do for this proposal specifically, is to talk about entropy of this proposal specifically. Discuss fingerprinting risk in more detail

JM: helpful

MD: we discuss fingerprinting risk in each instruction proposal, do you want summary?

GD: there is also guideline for webplatform, 3 things, what are we exposing, relative to web platform baseline, what are probability that applications run into it?

PP: helpful and can guide future proposals

ZA: Deepti if you can add that via an issue, that would be great
