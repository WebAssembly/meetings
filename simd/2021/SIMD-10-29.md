![WebAssembly logo](/images/WebAssembly.png)

## Agenda for the 2021-10-29 video call of WebAssembly's SIMD Subgroup

- **Dates**: 2021-10-29
- **Times**:
    - 4pm-5pm UTC (9am-10am PDT)
- **Location**: *link on calendar invite*

### Registration

You are a new attendee, please fill out this [form](https://forms.gle/9eB2ZYaziPEcTJabA) to attend.

## Agenda items

1. Opening, welcome and roll call
    1. Opening of the meeting
    1. Introduction of attendees
1. Find volunteers for note taking
1. Adoption of the agenda
1. Proposals and discussions
    1. Spec text changes (Zhi An Ng)
    1. Prototyping status (Zhi An Ng)
    1. Deterministic FMA
1. Closure

## Meeting notes

### Attendees

- Dan Gohman
- Marat Dhukan
- Zhi An Ng
- Deepti Gandluri
- Arseny Kapoulkine
- Richard Winterron
- Mingqiu Sun
- Johnnie Birch
- Thomas Lively
- Petr Penzin

### [Spec text changes](https://github.com/WebAssembly/relaxed-simd/issues/19#issuecomment-949025272)

Presenting Spec changes: https://www.ngzhian.com/relaxed-simd/core/exec/numerics.html#relaxed-operations

ZA: Suggestion to move in a different direction. Each relaxed instruction returns a fixed-size set of results. Results can be different depending on the inputs. The spec defines the meaning of “relaxed” in relaxed SIMD instructions.

AK: The spec says here’s the superset of the hardware behavior, this is interesting - when we looked at dynamic swizzle instruction, Power PC had some weird behavior, simpler for FMA. For bit selects, when you take a minimum of a NaN and something else. Does this lead to not supporting architectures, or block lowerings for some floating point instructions - how are we thinking about - do we need to have a rigid set of rules for some operations?

ZA: There is a risk that we’re blocking some architectures, the spec text is not fixed. For the set of results for the operations should be fixed at times

AK: For a given target platform, for a given set of inputs the outputs will be same. Non-determinism within the same execution

ZA: When we get to phase

DaG: in theory we want to support all specs, in practice not that possible, how much do we other arch to target this proposal

MD: only one we need to take care of is power, other arch are much less popular, or no SIMD yet, and when SIMD added, will take into account of Wasm SIMD and relaxed SIMD

PP: We’re looking into min/max - common things would happen where the sign would be non-deterministic. If we look at what other libraries or standards do, we would expect that it runs into everything

ZA: Relaxed operators themselves, the results can be non-deterministic. Fmul, fadd results can be non-deterministic, the operators themselves can be non-deterministic

PP: If you look at libc, you would likely support more architectures

RW: Could we do a category, if an ISA switched from one type to another,

ZA: Current spec doesn't say anything about hardware, you can switch between the verticals of results, as long as it’s consistent
RW: Can you have a compatibility mode for hardware? Don’t know that that would occur

ZA: Suggestion is looking at other architectures - PowerPC, RISCV,

AK: It’s an interesting instruction set, not sure how well it maps to the baseline SIMD proposal

MD: Riscv doesn't have the SIMD extension

AK: If the baseline spec doesn't map, then that’s a separate interesting problem

MD: Arm MVE is an emerging ISA, we could be looking at it. ISA for microcontroller

PP: RISCV you could say I want a certain set of elements and get it, you could emulate SIMD instructions

MD: Can’t do swizzle operations for example

ZA: General direction of hardware picking a certain set of results, are there any further concerns?

PP: The reality is that you have to define in the broadest way possible. NaNs and 0s are implementation defined, no matter which architecture we target it’s still going to work.

ZA: Comments on text and approach welcome, will be asking for Phase 2 at the next CG meeting. Any comments/concerns?

PP: If we do relaxed min/max this way - means that some architectures could return, instead of swallowing or propagation g it returns the first one etc. How do you think developers would distinguish between those behaviors?

DaG: Are there architectures that do anything other than x86?

MD: Didn't look at Riscv vector extensions, baseline returns MinNum, MaxNum. On x86, min/max instructions on SSE a < B return a, AVX512 does something different when comparing signed zeros it returns the right one based on sign. So even on x86 there are two different behaviors

PP: For swizzles we could do something different, if you don’t rely on out of range indices.. But for min/max it’s different. Do we expect developers to ignore ambiguity, or have semantic checks?

MD: Results are only guaranteed if inputs are guaranteed, only in case where precision to that level is not important, this is the baseline assumption for relaxed-simd

### [Prototyping status](https://github.com/WebAssembly/relaxed-simd/blob/main/proposals/relaxed-simd/ImplementationStatus.md)

JB: Do we need more than one implementation?

ZA: For eventual phase advancement to phase 4, not needed for phase 2

### Deterministic FMA

DeG: We used to push for having scalar versions map to vector operations to make sure there was a way for architectures that don’t have SIMD supported have a way to match the semantics with scalar operations, but none has used it, and it’s a maintenance burden, not sure about the ROI of a separate proposal that adds scalar versions

MD: for scalar version, we have add in Wasm 128 load zero and store lane instructions, these are supposed to be use for loop remainders, don't strictly need scalar ops for load remainders. Preferable to use SIMD instructions, behavior of SIMD differ slightly from scalar, users will be surprised if result in loop remainder is different. Supporting of FMA in Wasm, don't think should be part of relaxed simd or SIMD in general, unlike most other instructions, FMA is fundamental for floating point, similar to sqrt or div, in fact some CPUs don't have anything except for FMA, implement plus times divide using FMA. Different from other instructions, many of them only make sense for DSP and only exist for SIMD version. I will be supportive of Wasm proposal to just add SIMD and scalar of FMA.

PP: relax simd will pave the way for for det fma, if we can have relaxed, then definitely can have det. For scalar version of other instructions, in regular SIMD we have pmin/pmax, can be added to scalar, they can be used in some cases for faster min/max. Not sure how much we care about scalar floating point ops, everytime someone needs perf they will write SIMD. But for symmetry we can have the same ops and vectorize.

DaG: if using LLVM, and autovectorize, how will it generate a loop without the scalar

MD: LLVM can also use a SIMD instruction in place of scalar, most modern CPU run same SIMD as same throughput as scalar

AK: It doesn’t seem like something we want to do in LLVM. There are use cases where FMA scalar would be useful. Purpose of relaxed-simd proposal has been guaranteeing some form of determinism, but guaranteeing performance. There are valid use cases where scalar FMA is useful, and using vector FMA is worse. IF you’re paying 4 times the cost with an emulated FMA

DeG: from engine perspective, see how scalar fma can be a separate proposal, for the ones we have vector instruction, then try to map the scalar back, haven't seen a lot of value. Curious to hear what use cases you have in mind

AK: precise floating point algorithms, given a sequence of floats, give the exact value for sum, paraphrasing this. Vectorization there is difficult, may be possible in some cases. Even if part of it is vectorized, there will still be scalar parts. This is where FMA has to be actual FMA. 2 different types of FMA, don't care and want quick way, another type where FMA needs to be exact. Some algorithms are not vectorizable, you can use vectors, but waste lanes, and run into more problems. Might be useful to prototype, compare performance, but slightly dangerous route.

MD: Thinking about it more, on CPUs that don’t support FMA we may want to emulate with scalar FMA instead of vector FMA.

DaG: The value of using relaxed-simd when communicating with developers. It needs to be easier to communicate what the FMA story is - falling back to QFMA or not, it’s not clear what how we would explain this

MD: simd and relaxed simd focused on performance. relaxed fma fits this. deterministic fma is fundamental. will be understandable if it is not part of relaxed

PP: The output is different? Not only timing?

MD: there are intel cpus which don't support FMA, everything that doesn't have AVX2, most intel atom. those are sse4 only

PP: you'll be stuck, what's the solution for det fma?

MD: emulated

DaG: software yea

PP: if we do fma on arm, powerpc,etc, will the result be the same?

MD: with exception of denormalized numbers, results will be the same

DaG: exception behavior, not exposed in wasm

PP: Well if it’s consistent in terms of NaNs, we should be consistent across the board

MD: arm neon on 32 bit arm doesn't support denormals but support FMA

DaG: ARM Neon doesn't support it at all

PP: Wasm supports denormals across the board, why we have the min/max semantics that we do. If you look further in JS does it too
DaG: It also comes for IEEE754, we aligned the spec with that. The discussion is pretty old, 2008 was wrong, they decided to fix that but discussions were still in  progress then. But we aligned with that and now it’s live

PP: fma is not defined in JS, we can define whatever we want. Since we are forced to handle denormals, have to think about how diff arch handles it.

DaG: MD, do you know other platforms that have different subnormal handling?

MD: powerpc altivec will also have floats without denormals, almost all support vsx on power today, so not a big concern. Probably ARM NVE doesn't support it

DaG: It doesn't support subnormals across the board right?

MD: its microcontroller, not surprised if they don't support denormals

DaG: We already don’t support those platforms, because of the determinism discussion in the earlier SIMD proposal

DaG: on those, emulated if not supported. You can use qfma with special inputs, check results. Then you know if det fma is fast.

DaG: This already aligns with how different programming languages work, or they have a way of testing which intrinsic is fast, then they use the deterministic FMA operator.

PP: will be similar if you want to determine if relaxed op is similar to det.

DaG: standard library can't do that, they need deterministic fma to implement semantics

MD: If the users want to check whether FMA would be fast, then most likely they would like not use the emulated FMA

DaG: make sense to give them a way to check if fma is fast

MD: in practice i don't expect dev to write code on slow fma case

DaG: Depends on developers

TL: if language standard libraries need det fma, can we have standard library do this check? then fall back to their own software fma? rather than baking into Wasm in the engine?

DaG: Hadn't thought about that

PP: could be easier, or not. Not sure about the cases this would work. In some cases it will be slower. Depends on what else the library does, you can slow it down even further.

DeG: we talked in the past about is-this-fast primitive. Aside from FMA, it is easy for dev to query if something is fast, seems useful as a general primitive. The way we spec relaxed simd, doesn't look like it, but curious about other cases.

PP: can check if regular simd is fast, if not then can use the relaxed one. Personally think not a bad idea, give user a choice if they want to take this path or not. Depends if we are differentiating between different flavors or not.

DeG: it's binary, just fast

PP: you can build a profile based on this. that said, not sure if it's a big deal from privacy point of view. OTOH, it is generally useful to have something like this, people will still find ways to determine if instructions are fast or slow. there is a need to do this anyway, if we standardize it, might be a good thing.

DaG: FMA is kind of an operation that gets inlined, adding a load/compare/branch around every FMA may be an observable performance difference

TL: yea fair
