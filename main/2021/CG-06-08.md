![WebAssembly logo](/images/WebAssembly.png)

## Agenda for the June 8th video call of WebAssembly's Community Group

- **Where**: zoom.us
- **When**: June 8th, 4pm-5pm UTC (June 8th, 9am-10am Pacific Time)
- **Location**: *link on calendar invite*

### Registration

None required if you've attended before. Send an email to the acting [WebAssembly CG chair](mailto:webassembly-cg-chair@chromium.org)
to sign up if it's your first time. The meeting is open to CG members only.

## Logistics

The meeting will be on a zoom.us video conference.
Installation is required, see the calendar invite.

## Agenda items

1. Opening, welcome and roll call
    1. Opening of the meeting
    1. Introduction of attendees
1. Find volunteers for note taking (acting chair to volunteer)
1. Adoption of the agenda
1. Proposals and discussions
    1. Review of action items from prior meeting.
    1. Present and discuss [Interval Primitives](https://github.com/WebAssembly/design/issues/1384) (Kloud Koder)[40 mins]
    1. Present and discuss [Multiprecision Primitives](https://github.com/WebAssembly/design/issues/1386) (Kloud Koder)[10 mins]
    1. Status update on [Branch Hinting](https://github.com/WebAssembly/branch-hinting) (Yuri Iozzelli)[10 mins]
1. Closure

## Agenda items for future meetings

1. Present and discuss [Multiprecision Primitives](https://github.com/WebAssembly/design/issues/1386) (Kloud Koder)[10 mins]

### Attendees

- Derek Schuff
- Elle Imhoff
- Frank Denis
- Kloud Koder
- Chris Fallin
- Garret Gu
- Zhi An Ng
- Francis McCabe
- Thomas Lively
- Yuri Iozzelli
- Jacob Abraham
- Emanuel Ziegler
- Dan Gohman
- Keith Miller
- Sabine
- Alex Crichton
- Sean Westfall
- Alon Zakai
- Paolo Severini
- Lars Hansen
- Andrew Brown
- Sergey Rubanov
- Rick
- Ryan Hunt
- Asumu Takikawa
- Nabeel Al-Shamma
- Daniel Wirtz
- Sam Clegg
- Ross Tate
- Petr Penzin
- Flaki
- Zalim Bashorov
- Jakob Kummerow
- Richard Winterton
- Slava Kuzmich
- Yury Delendik
- Adam Klein
- Pat
- Jlbirch
- Vivek Sekhar
- David Piepgrass
- Andreas Rossberg



## Meeting Notes

### 1. Present and discuss [Interval Primitives](https://github.com/WebAssembly/design/issues/1384) (Kloud Koder)[40 mins]

TL: ultimately this is a performance feature, higher level languages would unlock this extra performance if Wasm supported these ops. Reminds me of the SIMD proposal, which was just standardized recently; in the early days of SIMD, the approach we took to justify doing anything at all was to get estimates of the actual performance benefits. Two parts: performance benefits that could be unlocked by baking these things into Wasm, and then the impact of the new instructions, what applications will use them. To justify all this work, it would be really good to have similar estimates of the performance and impacts on applications. On SIMD we took some native code that used SIMD, and looked at the performance difference when it didn’t use SIMD, used only instructions that were only available in Wasm, but native versions, and looked at the performance difference between that and when the code could use SIMD. Will be great to see a similar exercise done here to justify the additions to the spec.


KK: that would be creating a benchmark, and we could argue about how representative it is. If i did this it would use the intel instruction set. But you'd see a lot more benefit with e.g. nvidia which has native rounding mode. But that’s a hard hurdle to overcome.

TL: we also don’t have a precedent for these future facing features where we add something to Wasm with the expectation that hardware will support that and make them fast in the future. In SIMD it’s about what performance we can get now. To my knowledge, Wasm is not being run on GPUs in any significant amount.

PP: available on CPUs too, all the primitives are in standard math library


KM: i thought gpus don’t handle function calls in the same way CPUs do. Doesn’t that cause problems?

KK: what do you mean by function calls?

KM: there’s some Web Gpu stuff, whistle or something? [WHLSL] It can’t do recursion or functions must be inlined.

KK: yeah you’d run out of stack fast

KM: you can’t translate wasm directly into that, a lot of modules may not compile.

KK: would compile and run out of stack really fast but whole thing would crash. That’s sort of a parallel issue.

PP: could validate module, but GPU is different. C and C++ has all this facilities. Some of this is available in standard math library.

KK: yeah in x86or C they'd use this floating point environment stuff which is very dangerous. But the capability to change rounding modes is there.

PP: we can actually use this on CPU already. The comment that this is future facing and only on GPUs is inaccurate.

FM: AFAIK we haven't made any real effort to design wasm or run it on GPUs. so anything that refers to being better on GPUs isn’t really in scope (at least temporarily).

KK: agree, will emphasis on the word temporary, i think it is inevitable with the backing behind it

FM: I think it would be a completely different standard

KM: on FM point, since there isn’t production Wasm engine targeting GPU, seems premature to add instructions that benefits only GPU

FM: the other part is to pick up on Thomas’ point, to get convincing evidence, even a small benchmark would be a lot better than nothing

KK: would it be compelling if i were to do that on Intel, in order to change rounding mode, have to touch the control mode, will be a lot faster than in Wasm. I can do in Wasm, and in assembly, and measure performance ratio. Will that be compelling? Is that a good test.

KM: what we did for SIMD was… one of the main goals is to be portable. So for SIMD we tested on most of the major CPU architectures (a bunch of ARM and x86). We checked that it at least didn’t work. And it was focused on how you’d write SIMD in C++ e.g. the SIMD support available in the language rather than doing it in assembly.

KK: if i were to do the equivalent, i would be using some open source C++ interval library?

KM: yea something like that, where it generates actual… there isn’t native instruction, assume interval library is changing the rounding instructions already

PP: we tried to do this test: in SIMD we can process the same data in SIMD or not. But right now we don’t have rounding mode support at all. So this would enable it rather than just making it faster.

FM: another thing to try, taking one of the engines, say wasmtime, trialing the instructions that you want to do on that.

PP: sounds like making an actual implementation

KM where we don’t have a control. With SIMD, just don’t do the vectorization, just do it serial. But here, how would you do it? The main concern I have with rounding mode is that for a browser, every time you make a function call out of your module, you have to restore back to the rounding mode used by the browser, and it has to have the rounding mode that JS uses. Every time you call into another module, you have to set it, and on entry.

KK: but it wouldn’t, if you do it per instruction, it only affects the instruction, you wouldn’t worry about global state

KM: assuming every instruction in a function has the same mode, you’d be setting in a function. Not sure how that would perform.

KK: discussed this a bit in the issue, if you compile interval code, you want to group instructions by common rounding mode, but under this proposal, rounding mode only applies to a single operation, reset to default every time. If you use the existing instructions, nearest-or-even. If you want, up you use an instruction that rounds up.

KM: sure but once you compile it, you can see. E.g. on ARM the mode is per-instruction.

KK: which is exactly what we want.

KM: all Wasm needs to be portable, we have to make sure it isn’t horrendously bad on intel as well.

KK: yeah the irony is that it would be really bad compared to arm or nvidia. But in the absence of any support it would be even more horrendous. Also, regarding the control: it is possible to write correct interval arithmetic with nearest-or-even, you just get wider intervals. So we could hack that up. You could compare that against the same “nearest-or-even only” mode in assembly or C.

TL: seems like a good control to me

FM: we are not really comparing assembler with Wasm

KK: that’s why i said i can write it in C, actually C is quite close to assembler, GCC can optimize it well

KM: we try to avoid tests where people handroll assembly. We focus on Wasm being a compiler target, certainly possible to inline wasm in clang. Compiling a giant library is the general use case.

FM: there isn’t enough expertise in the group to have a clear idea of what the benefit will be, can you suggest a potential experiment that we can use to evaluate the benefit of this?

KK: performance benefit, or others?

FM: since this is a performance feature, then performance.  Generally we need to know what the benefit would be.

KK: transcendental function like taylor series, it’s extremely taxing on conventional systems

FM: what would a good experiment look like?

KK: computing a bunch of sin, cos, logarithms?

FM: what is the a and b?

KK: a will be compile to wasm, and b will be native

FM: that won’t be a good experiment

TL I think you’d want to compare native to native or wasm to wasm. Probably you’d compare native to native since it’s not in wasm yet. This is what we di din SIMD. in the control case youd limit to the capabilities already in wasm. So youd make a native build with just th eone rounding mode and build everything on top of that. Then the other case you’d use all the proposed capabilities, and find the performance ratio. And we assume you could get the same ratio.

KK: this is where comparison with SIMD breaks down. If i were to use nearest-to-even and compare to optimized impl with 4 rounding modes, it will be a comparison of precision and not performance. The real problem is that the way things stands, the sheer number of instructions will affect the performance… code verbosity is a roughly proxy for performance.

KM: can’t you keep refining it until you get to the precision that native has. You’re comparing the exact same results, you want both to return the same results. That would be apples-to-apples.

DS: we’ve eaten into our other allocated time, wanted to see if you want to continue this conversation or go to the next presentation.

KK: we can defer the next presentation. You want the test to come out with the same answer. The question is how much time, how many lines of code. Maybe the real apples-to-apples will be wasm as it exists, and a wasm with some support for the interval instructions, e.g. 10 instructions, and make a loop out of it, and compare performance that way.

KM: that sounds like a good test, but that seems harder than doing it in C, right?

KK: yea, someone would have to help me a lot about that, i know nothing about the guts of wasm.

KM: you do it in c you can get a lot of it without help

KK: write this interval arithmetic loop computing sin/cos in C, and what is the a/b case.

KM: the other case is using native rounding mode. One loop will use nearest to even, has a loop where it keeps refining, to get the same precision. The other is just natively.

KK: they both ultimately get the same exact answer

KM: right and you see what the performance difference is 

PP: handicap one side until it looks like wasm

KK: i’m not sure how to handicap it. The easiest way would be to compile to wasm? The deficit in performance isn’t so much due to the lack of rounding mode, but having to issue so many instructions that treat integers as floats and back.

KM: that’s fine, that’s exactly what you’re trying to show. Right now if you have to do that, you have to issue those instructions. What we are trying to show to motivate this is that by having thes support in Wasm, you see a similar improvement in what the experiments will show.

KK: so, but as petr said I’d have to know how to handicap my C so it works more like wasm?

KM: write a c library that does the repeated refinement. Don’t think GCC/Clang knows that you implementat a different rounding mode in floating point.

KK: I think i see what you’re getting at. So they end up the same result, and I do whatever i’d have to do to implement outward rounding.

KM: yea you should put a good faith effort to try and optimize, i can make any experiment an infinite loop. We want it to be a fair experiment, but that’s what everyone was getting at for what the first experiment will be.

DS: thanks for the presentations and all the suggestion

### 2. Present and discuss [Multiprecision Primitives](https://github.com/WebAssembly/design/issues/1386) (Kloud Koder)[10 mins]

[POSTPONED TO NEXT SLOT]

### 3. Status update on [Branch Hinting](https://github.com/WebAssembly/branch-hinting) (Yuri Iozzelli)[10 mins]

YI: [Presentation](https://drive.google.com/file/d/1NYFMqDtikAr7ClLK-4XUPQme_ftDZb3T/view?usp=sharing)

RW: where do the gains come from?


YI: for this test, i don ‘thave that data. I can get it I think because I can use perf on the generated code. On previous experiments, I think its both cache locality and register allocation. (this is v8; it didn’t generate very good code for this case before). I think it’s mostly cache locality, pushing these unlikely code down away from the hot code. But it does seem that register allocation is a bit better too, just from looking at the code. I can make the binaries available on the web.

RW: i will be interested to see where the gains came from, code locality… doubt it is the branch prediction

YI: <something> v8 issues branch hints, just generates different assembly code

DS: in general sense, there’s no good way for end-to-end test, non-observable behavior in the engine. One way in the standard unit testing system is to design a system with some other kind of side entry api that will let you observe these differences. In this case that could mean providing some way in the reference interpreter to observe some of these expected effects, not sure if that’s something we want.

YI: right now, for names section, it’s kind of similar, how is that handled in the test.

DS: having no test is another answer

DP (chat): Well, you could run all tests with and without hinting to ensure (1) same behavior and (2) branch hinting isn't slower

KM: is there a flag to tell the interpret to treat custom section failures as failures? Can give you an idea of how the custom section is parsed, you won’t test the actual feature of emitting the hints. I can see parsing tests, not sure about code-gen tests. Engines probably have a way to check these, e.g. assembler. Not really portable.

YI: will open issue, and see if there is easy way to add parsing test

PP: this can only be used from one particular machine simulator, no toolchain support or other stuff.

YI: at the moment here isn’t

PP: how is this supposed to work? Adding a feature to one interpreter.

YI: idea is to add support to toolchains, if writing C, there are builtins to add hints, in native code, LLVM has this and pass it to Wasm

PP: do we know who will use it

DS: in toolchain, llvm has this,exposed to languages, can use profile information to add hints. Ideally we can do code layout optimizations. It is a requirement for phase 4 for implementation in toolchain, when we get there, we can discuss if 1 interpreter is enough.


PP: yea, my concern is slightly different from what branch hinting in normal toolchain.

YI: i expect that for most code, in c/c++, even in native, those hints are not that effective, they are manually put there by programmer, they don’t know actually how likely. An interesting thing i see is that this can be hooked up to profile-guided optimizations, the hints will then be much more reliable.

PP: on native, we get profile, recompile. But if we run on x86, profile, then run on arm, what happens?

YI: the idea would be that you’d run it in the VM and have a way to get that information from the VM, it would be portable.

DS: we are over time, we’ll have to continue the PGO discussion later.





