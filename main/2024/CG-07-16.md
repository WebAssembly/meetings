![WebAssembly logo](/images/WebAssembly.png)

## Agenda for the July 16th video call of WebAssembly's Community Group

- **Where**: Virtual meeting
- **When**: 2024-07-16, 16:00-17:00 UTC (2024-07-16, 9am-10am PDT, 18:00-19:00 CEST)
- **Location**: *link on W3C [calendar](https://www.w3.org/groups/cg/webassembly/calendar/) or Google Calendar invitation*

### Registration

No registration is required for VC meetings. The meeting is open to CG members only.

## Agenda items

1. Opening
1. Proposals and discussions
   1. Phase 4 vote for
      [Exception handling](https://github.com/WebAssembly/exception-handling)
      (Heejin Ahn, 10 minutes)
   1. Vote to increase Web import limit to 200k ([issue](https://github.com/WebAssembly/design/issues/1520), Thomas Lively, 5 minutes)
   1. FP16 proposal: next steps (Ilya Rezvov, 45 minutes)
1. Closure

## Agenda items for future meetings

*None*

## Meeting Notes

### Attendees

 - Paul Dennis
 - Robin Freyler
 - Derek Schuff
 - Ben Titzer
 - Francis McCabe
 - Jeff Charles
 - Paolo Severini
 - Daniel Lehmann
 - Mats Brorsson
 - Nick Fitzgerald
 - David Degazio
 - Johnnie Birch
 - Ryan Hunt
 - Chris Fallin
 - Sam Clegg
 - Yuri Delendik
 - Daniel Hillerström
 - Chris Woods
 - Ilya Rezvov
 - AlonZakai
 - Jakob Kummerow
 - Ben Visness
 - Conrad Watt
 - Linwei Shang
 - Emanuel Ziegler
 - Deepti Gandluri
 - Jake Enget
 - Chris Woods
 - Alon Zakai
 - Matthais Liedtke
 - Julien Pages
 - Heejin Ahn
 - Adam Klein
 - Andrew Brown
 - Luke Wagner
 - Brendan Dahl
 - Alex Crichton
 - Michael Ficarra
 - Andreas Rossberg
 - Nuno Pereira
 - Richard Winterton
 - Dan Phillips
 - Keith Winstein



### Proposals and discussions

#### Phase 4 vote for [Exception handling](https://github.com/WebAssembly/exception-handling)

AH: Discussed in the June CG meeting: introduced exnref in the October meeting. In June, agreed EH was ready for phase 4 but needed some JS API tests. These have been added now.

CWoods: We’re about to add exnref to Wamr, is there a plan or a timeline for when the new EH will be enabled in emscripten, so we know what’s happening in the Wasi SDK, and the rest of the buildchain

HA: As soon as we go to phase 4, it can be available in VMs without a flag. Emscripten has support if you use a flag. If you use just upstream LLVM/clang, it does not have support yet. I am currently working on it. For wasi, I’m not sure about the status. BTW, Voting to phase 4 doesn’t mean we are removing the previous support anytime soon.

CW: We’d use the new implementation for code size, its just working out what’s required on the toolchain - we can follow up offline

HA: exnref doesn’t reduce code size.

CW: We don’t want the code maintenance costs going forward, will just keep the one code path

DS: Wasi SDK uses bare clang & LLVM and a pretty minimal libcxx changes - I assume once exnref is in upstream LLVM, it’ll be in WASI SDK pretty quickly.

CW: we’ve already ported setjmp/longjmp over to old EH, which has been useful.

BT: I was wondering the state of setjmp, lngjmp, will that be upstreamed into LLVM?

CW: good question. We have an internal port, but we should do that.

HA: upstream LLVM has sj/lj support, and it shouldn't really have to change with exnref, exnref is orthogonal to that.

**Poll:** Vote to advance the Exception handling proposal to phase 4:

|SF|F|N|A|SA|
|--|-|-|-|--|
|25|6|3|0|0|

#### Vote to increase Web import limit to 200k ([issue](https://github.com/WebAssembly/design/issues/1520), Thomas Lively)

TL: The limitation on the number of imports is currently 100k. These limits are generally meant for compatibility on the web so that the engines agree on them. But if real-world apps have a good reason to go over them, historically we’ve just raised them to allow the app to run. So the question is, is this a reasonable use case. The use case is that Photoshop is using wasm-split to get deferred loading. This is what David Sankel from Adobe came and brought up in the Pittsburg meeting. Our feedback to them was to use wasm-split. When they do that they end up with ~130k imports, where functions are split out from the primary to secondary module. Since this isn’t a human written API boundary you end up with a lot of imports. So i’d like to increase the limit to 200k to allow splitting very large applications without exceeding the limit. Details on the issue show various workarounds, indirection etc to reduce the number necessary. But I think we shouldn’t have to work around this in the tools and it’s reasonable to raise the limit.

CW: I’m totally happy to increase the limit, but: my first question is - are we in danger of hitting an analogous limit on exports? 

TL: good question, I’m not actually sure. I imagine if there is a limit we should keep them in sync. I’m not sure if there’s a limit or what.

BT (in chat): There is a limit, and it's the same.

CW: I support keeping them in sync and raising both. Also I don’t think we need to work around, but it’s about moving work from active segments to JS. it seems like you don’t need the limit if you move the segment work to JS, would that be better than having all the imports in the long term?

TL: We did talk about it when Ryan brought up compat import format - there is a requirement in the spec of the imports being looped over and how its hard to optimize. That said it would be hard to do that anyway when coming from JS - we’d need someone from the V8 team to confirm that

BT: if we’re going to 200k, we might hit the limit again, should we just increase it to the number of functions, 1M?

TL: I’d be ok with that.

JK: Regarding performance, it might be hard to estimate what the performance impact would be - iterating over 100k+ or a 1M of anything has a cost. The compact imports proposal if it does materialize, that would change the calculus

CW: would you have any concerns to rasing import/export limit to 1 million?

JK: I don't think we’d have any issue with 1M. We might hit a limit at ~16M. Obviously it could be slow to iterate over that many on instantiation, but I assume if you create such a big module, you are willing to pay for that.

RH: We don’t have any technical reasons not to do it - it’s more a social reason

BT: My reason for matching the number of functions is that there is a nice symmetry. You could have passthrough which imports all the functions and exports them all again.

TL: Does the million include imported functions? Not clear. I guess if you can define a million functions maybe you want to export them all.

BT (chat): I think the maximum number of functions limit is for defined functions.

TL: Anyone object to raising both the import and export limits to 1M?

No objections raised.

#### FP16 proposal: next steps (Ilya Rezvov)

IR presenting [TODO slides]

Performance numbers

AB: help understanding the table: the RH column, is that speedup or slowdown? (slowdown) for F16 GEMM or M32 GEMM are those 2 different algorithms? Or are they just multiplying different matrixes

IR: not totally different algorithms, i just compared performance of float operations, measured in FLOPS

AB: in the first row you multiply an f16 matrix and in the 2nd row you multiply an f32 matrix

PP: Its the same algorithm, but different 

CW: i would be surprised to see more than a 2x speed up, but I guess the number of lanes is 2x

IR: less memory, better cache optimization helps too

PP: (missed in the notes)

DG: Question for you: did you run this by the XNNPACK team? Some of these are surprising. They’ve done a lot of native benchmarking, did you check this with them?

IR: To clarify, it’s not running native F16 GEMM, its F16 GEMM compiled to wasm on hardware with native F16

AB: to go with that, did you compare the pure native version of this benchmark with the compile to wasm version of this benchmark?

IR: no

AB: would be interesting to see how far we are from true native

NF: wanted to clarify, is native F32, that’s wasm running the F32 gemm benchmarks, not using F32 to implement the F16 benchmark? So the most direct comparisons are native F16 and F16C and software?

BT: do you have more insight into why F16C is so slow?

IR: Unfortunately no, there’s nothing 

CW: also related to F16C+AVX column. Is this the strategy older AVX versions of X86 will need to use to emulate F16? So you’d never really want them to generate F16 in the first place you’d rather have them using F32?

DG: In F16, where in V8 is that implemented? Some of that is only in Liftoff.

IR: all of these are liftoff

DG: That’s not always a fully accurate comparison. It’s still not the optimizing compiler, so comparing baseline to native will be helpful

IR: yeah we probably need to implement turbofan for x64

DG: I mean, these are all wasm numbers: how they compare to native just in terms of the factor

CW: To expand on the earlier point on the F16 emulation. If you don’t have native F16 on your X64 you really would want the website to use F32 wasm instead of F16 wasm. So as an ecosystem do we want to encourage sites to use F16 if they can’t predict which platform they’d be running on

IR: So that's the next topic. Because of those I think we need a way for the app to figure out if the implementation is fast enough to run it. It could of course run anything for portability reasons, but for real world you’d want to know if the platform supports it. But I think you’d want the app to run in different ways based on the hardware. So to keep the performance predictable, I think we want to introduce some way of using emulation detection. [predictable performance slide]

IR: IMO this machinery solves the issue raised that we need feature detection support. Are there other opinions?

AR: Different definition of predictable performance - you’re saying you can reflect on the environment and know whether it supports the feature.. But  I meant to say that when you generate some WebAssembly code, you have a rough performance model for what you have in mind, and that it’ll map predictable across hardware. Anything that has feature detection in WebAssembly is a red flag for me. It's a sign that it's premature to add to the language.

CW: The numbers for the emulation make me wonder, are we just adding FP16 a couple of years too early? It seems very different from how we’ve done this before, running ahead of the de facto hardware landscape. Not that we could never do this but it does give me pause.

DG: I agree with you on the basis of these numbers, but we need more introspection about whether they are actually representative. We need to go back to partners who were seeing benefit.
It’s more analyzing why it’s slow, i’m not sure these are fully reflective of the actual hardware.

CW: I would in particular be curious about whether the partners are excited about speeding up like 10% of their users or if they are expecting across-the-board speedups.

DG: I don't think it will be a majority of the user base, but a percentage. It won’t be consistent across all hardware,but it will be a set. There’s excitement about F16 especially on mobile. It will be interesting in how we slice it. We’ll have to think about where we draw the line, but it’s premature until we know better where and how we’re falling off the cliff.

CW: I think that's the best possible answer we can get at this stage. I appreciate that you're going to get more information.

DG: yeah, this comes from an explicit request for CPU inference and there’s a class of hardware that’s ready for it. And we’ll have to slice the numbers. And we’ll want apples to apples, and also native vs wasm. If we see F16C+AVX get to maybe 6x, that’s still very different from 100x

AB: It looks like the slide is proposing feature detection more broadly than just F16. We've had those discussions in the past. Why did we abandon this in the past?

TL: Those past feature detection proposals were different, there were standalone proposals so a little different. Conditional sections was way too general, there were a lot of design issues that came up because of that. The second was conditional blocks, much less general and targeted toward SIMD use cases. But because it was standalone there was an expectation that it be more general, so it kind of died because it wasn’t general enough. So I think that leaves room for more specific feature detection that goes with a particular feature, so this is a new kind of feature detection.

DD: I concur on the performance numbers we see are realistic, the emulation cost is very surprising especially for F16C+AVX
Also concur that it seems a little premature if hardware is not really in a unanimous place. My position and Apple's position is that if we want to open the can of worms for feature detection or if we want to have this unbalanced performance, then we want evidence that it's uniquely useful. Is there a reason this has to be done in WebAssembly rather than WebGPU, which already supports float16. Why does it specifically need to exist in Wasm?

DG: the reason for adding some of these ML features to wasm is that what we see is that a lot of the production code is still running in wasm. WebGPU is a good fit, but a lot of those backends are experimental. The other use case is small models, and GPU bringup is expensive, so it’s more efficient to run it on the CPU. Also useful as a fallback for GPU especially for low end devices. There’s also a use case of wasm as an entry point where we want to minimize JS (JS has no F16 array yet)

CW: For the interop use case, are the conversions enough, or do we also need the lane operations that come in later architectures?

DG: for the interop bit I think it’s just the conversion we care about. For the running part we care about the other things

DD: conversion seems much easier, it's much more widely supported and more direct. From the perspective of models not adopting it yet, I don't think that's very compelling because it's so early and it's possible that WebGPU adoption picks up later. The point about smaller models does seem compelling though.

DG: There's also an inherent use for running intensive compute on the web, but we haven’t yet seen how it will break down in the wild, the split between wasm and GPU. There may also be NPU usage on the web, but the base case will stay relevant. Also there are non-massively-parallel models. Segmentation isn’t as extremely computationally expensive for example, and it doesn’t really need the GPU. so part of it will always be relevant.

AR: I feel this is premature and I think other people do too. I think it's important for WebAssembly to provide features for newer features that are not ready to be added to core Wasm. We should look at opening ways for wasm to have features that are not ust extensions of the instruction set, for quicker turnaround, more experimentation, and deprecations. We talked about builtins for that in the past. We can use that as a first step to iron things out and mature over time, and then in a couple of years we see what CPUs do and how people use it, then it could maybe move into core wasm. As skeptical as I am bout ths, I don’t think we should block it, we can hopefully move it forward in a safer way.

AB and ChrisW and PP (in chat): +1 for builtins.

IR: that’s what I’m calling the import based approach, there’s an example on the slide that we di dn’t get to, but we can follow up.



### Closure
