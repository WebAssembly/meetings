![WebAssembly logo](/images/WebAssembly.png)

## Agenda for the July 2nd video call of WebAssembly's Community Group

- **Where**: Virtual meeting
- **When**: 2024-07-02, 16:00-17:00 UTC (2024-07-02, 9am-10am PDT, 18:00-19:00 CEST)
- **Location**: *link on W3C [calendar](https://www.w3.org/groups/cg/webassembly/calendar/) or Google Calendar invitation*

### Registration

No registration is required for VC meetings. The meeting is open to CG members only.

## Agenda items

1. Opening
1. Proposals and discussions
  1. Ryan Hunt: Present on [compact import section format](https://github.com/WebAssembly/design/issues/1514) and phase 1 vote. (30 minutes)
  1. Continue discussion on spec complexity from [in-person CG meeting](https://github.com/WebAssembly/meetings/blob/main/main/2024/CG-06.md#thursday-june-6) (30 minutes)
1. Closure

## Agenda items for future meetings

*None*

## Meeting Notes


### Attendees

- Thomas Lively
- Derek Schuff
- Conrad Watt
- Benjamin Titzer
- Jeff Charles
- Petr Penzin
- Ilya Rezvov
- Paolo Severini
- Francis McCabe
- Zalim Bashorov
- Dan Gohman
- Nuno Pereira
- Ryan Hunt
- Heejin Ahn
- Brendan Dahl
- Nick Fitzgerald
- Luke Wagner
- Jake Enget
- Chris Woods
- Manos Koukoutos
- Mingqiu Sun
- Keith Winstein
- Jakob Kummerow
- Linwei Shang
- Ben Visness
- Michael Ficarra
- Julien Pages
- Emanuel Ziegler
- Andreas Rossberg
- Andrew Brown
- Brendan Dahl
- Slava Kuzmich
- Richard Winterton
- Kevin Moore

### Proposals and discussions

#### Ryan Hunt: Present on [compact import section format](https://github.com/WebAssembly/design/issues/1514) and phase 1 vote. (30 minutes)

RH Presenting [slides](https://docs.google.com/presentation/d/19aZW4pAtPDc6Wgw6XGCBcR9f3DZ95LQm6ORYqLayJNw/edit)

Problem: the module name field of the import descriptor is repeated for every import. For most modules, the module name field is the same for most or all of the imports. This wastes space!
With JS string builtins, there are a lot more imports, all with the same module name.

CW: do you anticipate any need to do something different with the “read the imports” step here or is it just hypothetical?

RH: I imagine that we would change the "foreach" in the algorithm to do the "Get" for the module object just once. So say you had 1000 imports for the same module, we'd have to do 2k gets for the module object, and then again for the fields. Now you’d do one get for the module, and 1k for the fields.

CW: Is it really so observable that you can't just do that as an optimization?

RH: It did stop me from doing the optimization because of the complexity. It is in principle possible to fix this on the VM side, but it has come up in practice on the JS side that you really do have to follow the spec more strictly and do the right amount of lookups. I have seen proxies used here, we can’t just disallow them. I am open if people have opposition to changing the text and binary format, but I like the idea of doing it this way.

TL: In the dynamic loading use cases where we lazy load secondary modules, the proxies are very useful for that. Not for the modules, but for the fields. If we extend wasm-split to load arbitrary numbers of modules, we’d probably use a proxy for the modules as well.

AR: maybe getting into bikeshedding, but there's a close connection to what's happening in the component module extension where you have this idea of instance imports. This is kind of what that is, where you import a whole instance and you list the fields. So structurally you could make this a new form of import instead of a new section. The CM gives an instance type as a different thing to define separately. But treating it like an instance import would be logical in the text and binary formats. It would be ust a new form of import rather than a new section.

RH: I have a slide that imagines something close to that, but in general i dont’ have all the details worked out yet. So if there’s unification with the CM, we could do that.

NF (chat): What if we had a new import kind that was a vec of sub imports that only provide field names and not module names? that could fit well with the existing binary format and wouldn't need a new section. and maybe would be something we could naturally fit into extending the existing "read the imports" algorithm.The weird thing is what to do with the outer import's field name. that could either be ignored or concatenated with each sub import's field name, which maybe allows more deduplication, or at worst the outer import's field name could be an empty string. (this is sort of similar to the instance imports as well.)

RH: there was some reason i thought that wouldn’t work, now I can’t remember. But if we could make it work, that would be ok.

LW: It's because the two strings are hard coded into the import format before you get to the import description.

NF: you could require it to be an empty string, or ignore it, or concatenate it. There are several options

CW: I think the exact binary format can be something we can bikeshed offline on GitHub.

MF: Can you show the algorithm slide? My understanding is that this is the main motivation rather than size. Why can’t we just make the change, if we have 1k imports, to just have 1001 get operations? That would be observably different but, the question is whether anyone is relying on this. I've used a proxy in this position before, but they haven't returned different import objects each time. Why can't we just make the change?

RH: I agree, I hope no proxies are actually relying on this. If we want to change the algorithm we'd need an earlier step that iterates over the modules and does some sort of manual grouping. Maybe you could have some sort of map that caches previous ones? Not sure how that would work.

MF: Yeah, I was thinking of caching the module objects.

RH: I do think it’s possible, but it puts complexity in the engines to find the groupings, and it also doesn't address the fact that engines have to have this in uncompressed memory, so I still have a preference for extending the imports.

BV (chat): is there a limit on the length of an import name? If so, since the first byte you see is the length of the module name, maybe you could use that as a sentinel value for a different type of import.

JK (chat): no, no limit.

BV: Just wanted to emphasize that changing the binary format would guarantee that the imports are grouped, which would also speed up encoding. Not a huge deal but making it explicit is nice.

RH: There's a cost to keeping it all in uncompressed memory. A bunch of minor things but could be eliminated with a small tweak

CW: I just wanted to bring up a more restricted form of this. You could have a different import section, but then whichever one you see first sticks and seeing the other afterward would be an error.

RH: i’d be fine with that too

RH (continues to "extra" ideas slides)

JK (chat): another "extra" idea, that would save even more module size is if not repeating the type was also supported. I.e. some way to say "here are 100 imports from module 'env', they all have type 'externref': [name1, name2, name3, ...]"

RH: I thought about that too, I’d be open to that. Would be useful for decoding because you can save some repeated work. Not sure how it would work for combinations of things, would we have a combinatorial explosion where we repeat the type, but not that name? Could be interesting e.g. for the JS string builtins case where they are all the same type.

CWoods (chat): Linking via ordinal value, is something that has been done in Embedded OSes - like EPOC / Symbian. Works ok, and does reduce binary size.... but maintaining inter-module alignment between versions becomes hard, as the index order can change.

JK (chat): function locals already support that, so there is precedent :-)

AR: we discussed memoryref, tableref, etc where we have to type all of them, and name them in the imports.
We could use those in the import definitions the same way we already do for function. So we could do something in a more structured manner that fits into the broader picture in the future.

RH: This is a good opportunity to leave encoding space for things we think we'll need in the future.
Should we do a full poll or unanimous consent?

TL: unanimous consent seems ok for phase 1. Are there any objections to moving this to phase 1?
(none). OK, thanks Ryan.

#### Continue discussion on spec complexity from [in-person CG meeting](https://github.com/WebAssembly/meetings/blob/main/main/2024/CG-06.md#thursday-june-6) 

PP: AR, can you quickly recap some of what you presented?

AR: Sure. (type hierarchy slide) Some of this complexity we expected, but some surprised me, e.g. this type system I discovered when working on the spectec version of the spec. Then discussing SIMD features and the random holes, DG made some points about how it came about. I didn’t mean to blame anyone but, but wanted to highlight this. How leaving things out can actually globally increase costs rather than reducing them. With storage we did a good job where we had a long term plan to fill the holes in the future.

DG (chat): Where does numtype occur in the spec?

CW (chat): https://webassembly.github.io/spec/core/syntax/types.html#number-types, although spectec might rearrange things slightly

DG (chat): Is there an example of a concept that's common to all of i32/i64/f32/f64 but no other types?

DW (chat): I think there are different ways it could be factored - are you thinking vector types could be merged with number types?

DG (chat): I'm not making a proposal here; I'm just curious about what shapes the type graph.

CW (chat): To some extent it's probably inertia from 1.0, where "numtype" was essentially all the value types. When we added vectors there was a choice of how to add them to the hierarchy - we could have chosen to add them at the same level, but kind of didn't - I think because they have a suite of different load/store instructions?

CW: One thing is that there's tension between being uniform across the features versus respecting the actual behavior of the chips we're trying to abstract over. To me that seems to be the main objection to f16. It's not spec complexity, it's support for chips in the near term.

DS: In the previous SIMD proposal, we held that line and only included instructions that demonstrated speedups on all architectures.

PP: one of the holes is i8 types, and there are issues with overflow there. But yeah mostly because there’s a gap in the hardware.

AR: We talked about that last time. There's a trade off between having functionality that you want, with granularity and spec simplicity. Many competing goals. Portability as well. In the past we have gravitated toward one corner of that triangle and maybe hasn’t been super balanced with the others, and sometimes there are value judgments. For me as a spec person, I definitely observe a lot of complexity with SIMD in particular that causes a lot of work for me. Again with SpecTec, making things more precise makes the irregularities more apparent. We papered over in the paper spec, but when you have to make it more formal, it comes out. Deepti made the point that this is a problem that comes up in implementations, too. Maybe if there's just one gap in the feature matrix that we can't support efficiently, maybe we should just fill it anyway and the problem might go away over time. Or in the other direction, if there are too many holes, maybe we say it's just too early to add that feature.

CW: I would err more on the side of the latter because putting ourselves at the mercy of future hardware puts us at risk in forward compatibility.

AR: I would too. Maybe being in the middle is the most unfortunate 

TL: If we wait for all underlying architecture to fill in the gaps, we might be waiting for a long time or forever. We can’t make the architecture designers fill those gaps, there’s not much evidence that they even want to. Hope is not a strategy.

CW: But even then, do we want to eagerly spec instructions that are not supported by the underlying architectures?

DS: Goes back to the question from Pittsburgh. Are we abstracting over other architectures only, or are we defining our own architecture? Are we ok with filling gaps? I agree that if we wait for the gaps to be filled below us, that's no good.

PP: If what we think is going to be added eventually might not always be what’s added. With RISC-V vector spec there was an implementation based on in-progress spec, and then the spec went and changed, and now there’s 2 incompatible versions. At the end it’s sort of a balancing act; we can’t wait until there’s no holes but if we don’t want to commit to something that would not happen in the future even though it looked possible at the moment.

CW: I would err more toward the side of stability.

AB: Some of the assumptions we’re making is that everything that exists, will continue to exist, may not be the case

AR: In the early days of Wasm, we very consciously took a conservative approach. We only added things that were common to all modern CPUs that we knew was going to be permanent for the foreseeable future. With SIMD, we left that path and started to drift to the other side.
There is always pressure for features, but sometimes it’s important not to add everything, for the good of the project

AB: Adding 128 bit SIMD, when added, isn’t what the user truly wants when the users truly wants, really they want the max vector length of what they’re working on. We tried to find a common ground but didn’t really give them the feature and it got watered down too much. Wondering if there’s another way to get at the underlying performance that’s there.

PP: Plug for flexible vectors proposal!

AB: Any kind of a new instruction that’s supported differently on different architectures. We’re limited on what we can do.

CW: It's possible that we need to think more about WebAssembly + the host environment as a tuple that people are targeting.

PP: During the relaxed SIMD, there was an idea that we had 2 possible behaviors and spec them both, and one is accelerated. The ARM style vs the x86 style, you can possibly write them in the standard and based on which one you can take that. Someone added another idea that we could maybe express hardware accelerated things in wasm, and they turn into hardware instructions, or turn them into standard wasm. Also its easier to define the behavior when they are expressible in wasm, it can be tested, etc.  i don’t have a solid proposal of how this would look like having something like this reduces the pressure to add more and more instructions.

CW: Maybe where the group has evolved that we didn't have before is the solution for stringref.

AB: What's that approach?

CW: basically a limited form of compile time imports.

TL: There's a big difference there, where with JS string builtins, we are importing something that’s already in the embedder. Doing similar things for SIMD would be tantamount to bringing back SIMD.js and importing that in wasm. Everyone was really happy not to do SIMD.js, and do it in wasm instead. So I'm not sure that’s the best direction.

AR: I don't think you can make that comparison.

PP: you can have these imports be internal to the wasm engine, as opposed to adding that to JS

BT: I think what Wasm is depends on your perspective. If you look up from the hardware you think that wasm should express what you are offering. If you're software looking down, you want an idealized abstraction over the hardware. There are multiple levels of the software that you can think of. Maybe you want it to express the exact instructions that you write, and maybe you want a higher abstraction. We are maybe moving in a direction with string ref that you’re not talking about in terms of hardware. The abstraction mechanisms let us experiment ,and maybe we decide at some point that something belongs in the core, and then we can put it in. we’ve had various priorities for doing that, for SIMD we wanted to deliver on the promise of not having SIMD.js and have it in WebAssembly.

CW: Speaking with some others about this, from our POV , im looking from more of an ecosystem POV where silicon vendors can innovate, and we still maintain portability. E.g. here’s a function, it can have a hardware version but still embedded in the binary in core wasm, you can run it either way, and there's portability. It creates space for innovation without taking up a lot of work in the standards space, but enables ways for vendors to look at how they can accelerate the bytecodes. I know we're not the only ones to run into this. But e.g. using Wasm for SSL is slow compared to native because we can’t use hardware acceleration . That's where we're trying to find alignment between ecosystems to get both portability and innovation.
 
PP: Sounds like a limited form of compiler imports.

AR: Ben mentioned that something more high level is a higher level of abstraction: abstraction is hard, designing a good one may take more iterations of trying it out in practice than some obvious instructions that map to existing hardware. So maybe that's another reason to go with this form of imports because it lets you try out more things. This isn’t similar to SIMD.js because the problem wasn’t the instruction set but because the language was so remote that wasm was a much better palace. I think this problem will come back to use, we always get asked what about GPUs? Wasm is a CPU abstraction but GPUs get more and more important. How can we enable experimentation with domain specific things before they are so established that we can put them in wasm. Maybe static imports can help us try things that are not yet widely available and might go away, maybe that can reduce the risk and also help move faster with a lower bar than with core wasm features. So now that we’ve added machinery for these imports, maybe this is a good escape hatch for domain specific hardware.

AB: Two thumbs up for an escape hatch for this kind of work. The architecture designers are also working under enormous constraints.

AR: I'm sure they do design by committee too, to some extent.

### Closure

