![WebAssembly logo](/images/WebAssembly.png)

## Agenda for the February 23rd video call of WebAssembly's Garbage Collection Subgroup

- **Where**: zoom.us
- **When**: February 23rd, 5pm-6pm UTC (December 15th, 9am-10am Pacific Standard Time)
- **Location**: *link on calendar invite*

### Registration

None required if you've attended before. Fill out the form here to sign up if
it's your first time: https://forms.gle/JehrAB4gWbtHjybt9. The meeting is open
to CG members only.

## Logistics

The meeting will be on a zoom.us video conference.
Installation is required, see the calendar invite.

## Agenda items

1. Opening, welcome and roll call
    1. Opening of the meeting
    1. Introduction of attendees
1. Find volunteers for note taking (acting chair to volunteer)
1. Adoption of the agenda
1. Proposals and discussions
    1. V8 status update [Jakob Kummerow]
    1. Spidermonkey status update [Ryan Hunt]
    1. Binaryen status update [Thomas Lively]
    1. Wizard status update [Ben Titzer]
1. Closure

## Meeting Notes

### Introduction of attendees

- Thomas Lively
- Sergey Rubanov
- Francis McCabe
- Sabine
- Tim Steenvoorden
- Zalim Bashorov
- Ryan Hunt
- Adam Klein
- Ross Tate
- Rick Battagline
- Jakob Kummerow
- Lars Hansen
- Paulo Matos
- Conrad Watt
- Luke Wagner
- Slava Kuzmich
- Daniel Ehrenberg
- Asumu Takikawa
- Dmitry Bezhetskov
- Ioanna Dimitriou
- Igor Sheludko
- Ben Titzer
- Manos Koukoutos
- Keith Miller (Apple)
- Wouter van Oortmerssen
- Andreas Rossberg
- Emanuel Ziegler
- Deepti
- Yulia Startsev
- Zhi An Ng

### Presentation: V8 status update [Jakob Kummerow]

TODO(Jakob): slides

AR: Performance is not quite there yet. Do you have an idea of why?

JK: too early to say something definitive, only just started investigating. One reason is the various checks that we do. The JS versions is good at optimizing away type checks, we spent a lot of time doing that. In Wasm GC we do a lot of subtyping and null checks. Unclear how much the difference is due to such checks. We will learn more.

BT: I would imagine that the languages compiling to Wasm GC benefit from inlining and that V8 is not inlining yet.

JK: correct, we do not inline yet, we assume module producer does the inline. Have not looked at how much difference that would make.

BT: question about testing strategy. There are no tests in spec repo for gc, did you write your own?

JK: Yes, we have our own tests. We are prepared to fix bugs that toolchains find.

BT: i have an implementation in wizard, i wrote my tests too, will be useful to get some standardized tests, the blocker is getting it into wabt?

JK: that would be the most maintainable. My most recent devtools related work, i’ve been adding wasm modules by hand (array literals), because no tools readily available that supports the feature

AR: should at least implement binary to text conversion in reference interpreter to make that possible

JK: that would be useful

RT: how are people implementing dynamic dispatch in GC? Func refs?

JK: not sure

RT: how are you implementing func refs currently?

JK: Need to abstract over many different kinds of functions that are the actual backing. It could be a JS function imported, could be a function on the same Wasm module, or an imported Wasm module, which in turn could have any kind of backing. That makes the implementation of func refs and call ref instruction fairly complicated and probably fairly slow.

RT: Kotlin is probably going from getting all the JS optimizations to getting a lot of indirection, so I was just wondering...

JK: For first version they are planning to use tables with function indices, rely on classic function calls

SK: we use table with anyfuncs

AR: Why would call_ref be slower than call_indirect? It’s essentially doing the same thing.

JK: i think it should be comparable to call indirect

BT: If you just have to do a signature check, it should be faster. No bounds check.

AR: unless it is a type table then that’s not the case

RT: call indirect, will it be fast? I don’t know if either solution will be all that great

AR: if you directly call an import, it’s no diff from an import, you don’t know statically what it is

BT: call indirect that goes through table, has to bounds check, load signature, and may have to load the instance too, ~12 instructions. Call to import, it loads 2 things from table, no bounds check. Call ref should be indirect load from ref you got, maybe null check, and jump to that.

AR: why is call ref slower than indirect call?

BT: They should both be about 3-4 instructions.

RT: the ref has to contain module instance as part of the closure, direct has module instance. Call ref has an indirect reference.

AR: no, why should it need to know

BT: both should be about 3-4 instructions

JK: looking at last implementation, we need first 4 load then branch into different cases, and then do another 2 - 4 loads, until actual call then we can jump to it. Just to give you a rough idea.

AR: for an import, you presumably have to do something similar

JK: it’s modelled after call indirect. Call ref has a bit more work to do

AR: direct call to import

BT: instance pointer floating around in register, table off of that, and table has 2 entries, instance and actual target code, you load those 2 things, then you jump. If it is a call to Wasm, directly there, if JS then land in adapter

JK: diff the call ref is not in any table at all, could be coming straight from JS as an object, or load from global or something, that does add more work for the engine

AR: why wouldn’t you do that work when you pass the ref from JS to Wasm, as a kind of conversion

LW: semantically required, difference between JS func and exported Wasm function. Only latter can be directly passed. YOu can’t assign a JS func with table.set, that will throw

JK: you can create that on JS side and put it as arg, it doesn’t show up in any table

LW: you only ca

### Presentation: Spidermonkey status update [Ryan Hunt]

RH: We are also working on GC in Spidermonkey behind a flag only available on nightly. Based on old implementation of structured types. Working to catch up with V8’s milestone 3. Targeting baseline compiler rather than optimizing compiler. Working on structs, arrays, rtts, storage types, eqref, and dataref. Should be ready soon and we will provide updates as things land. We do not initially plan to implement low priority parts of typed function references. We do not plan to do i31ref to start. Open question about type canonicalization to start.

AR: the func ref proposal dropped any form of subtyping on func types.

JK: func bind listed as potential optional future proposal, not implemented yet

BT: implemented, found an issue, filed something, fine if it’s move to a future thing

CW: given the feedback implementers have given about type canonicalization, i’m worried that we implement the proposal minus type canonicalization, there’s no reason for implementers to shift because no one relies on canonicalization, [if this happens] we will want to redesign the proposal in a more principled way.

DE: Can you say more about implications on the design?

CW: If implementers don’t want to implement canonicalization, we would probably want to go to explicit nominal types with a parent declaration for each struct.

RH: if you don’t implement type canonicalization, you can have something workable for basic tests to get things running. Type canonicalization is useful for cross module scenarios, to get things running and performance data to see if we are on the right track, it is not on a critical path. I am interested in implementing, but it is a lower priority. Agree that if it turns out to be unnecessary, it may make sense to rework different aspects of the proposal.

CW: We don’t want to get stuck with a half-assed version of the current proposal.

JK: on board, we have no intention to block that aspect of the proposal, it is a question of prioritization, decided to focus on what’s important to get data. There’s been a lot of discussion about the type system, there seems to be a chance that if we did the work it will be ripped out later. If there are use cases, we will implement it to match the spec.



### Presentation: Binaryen status update [Thomas Lively]

[slides](https://docs.google.com/presentation/d/1yPcThMN-XuzLXU_ubg0wlhV3Q1BsYP4VVxDJA3ppvIk/edit?usp=sharing)

AR: non-funcref tables, already part of func ref proposals right?

TL: yup we still don’t implement them, also type imports exports also not yet implemented, people doing prototyping are not doing modular compilation yet.

DE??: is struct and array subtyping planned to be implemented?

TL: Yes, there’s no particular reason that hasn’t been implemented yet. Mostly an oversight.

ZB: plan to convert wasm with gc to wasm without gc?

TL: no concrete plan, but will be interesting to convert, probably doable, probably a good match for Binaryen, if anyone is interested to work on that, happy to work with you on that


### Presentation: Wizard status update [Ben Titzer]

BT: presented back in Nov, build a Wasm engine to do research on Wasm, not as fast as production. Interpreter tier only. Thinking about runtime systems developed on top of this engine. Implemented funcref and gc proposal together, union. Safe language, already GC-ed. Trickiest things were to get recursive types to work properly, and rethink how the engine represented modules, so that forward refs to get recursive types works. I did not implement rtt  canon. Implemented everything else, maybe except let. Implemented Java on top of GC proposal, lower to GC proposal. If you look at source of wizard engine, you can see the lowering for all of java bytecodes, enough to implement Java’s object model. Funcrefs for vtables. Rtt canon not necessarily for Java, nominally typed. We need a use case for rtt canon, we only have languages that don’t need that, we got at an impasse. Agree with the view that we shouldnt’ ship something that is not used anywhere, need a use case to motivate that. I can go up to work on Java, or going down to make the engine better, new execution tier to make wizard faster. If you implement java model on top of Wasm GC, space overhead, Wasm has headers, Java has headers for objects. My GC language has headers too, so now there are 3 layers of headers, 6 words of header for every object. The Java object has 2 words of headers, do we need a mechanism in Wasm GC (past MVP), where user level language can combine some meta information with Wasm engine in the engines, and also can I combine Wasm headers with implementing language headers.

JK: the idea to share the has code between running module and engine is an idea that came up before, can make sense as a post-mvp follow-up proposal. It affects only 1 field, but it should be a straightforward field to allow deduplication. In V8, we came upon this for map usage. If we want to use wasm struct/array as keys in JS weakmaps, then we need a hash code. If that module come from Java, then we also need hash code to support Java semantics.

BT: several of my techniques are exploiting 64bit machine to make header word smaller. If limited Wasm types, put them in a table, use only 20 bits, now you have 44 bits for other things. It is reasonable for engine to pack 1 32-bit user level field into header, probably not more than that. If we want to shrink things down, need actual language mechanism.

TL: is hash code something user visible in JS? In Java every object comes with identity hash code fixed for lifetime and not changed.

RT: JS or Java

TL: JS

JK: completely internal

BT: JS has weak hashmap

AR: objects in JS can also be keys in strong map

BT: map actually keeps them alive. In Java, without moving gc, use address of object. With moving GC, you have bits in header indicating if it has been moved or not moved, if moved the hash code hiding somewhere. Ultimately you need some space somewhere.

ZB: In Kotlin, it is important to have cheap access or calls for nullable refs, and any other language with nullable types.

TL: JK talked a lot about cost, what goes into a call ref, is it on the roadmap to rework that and make it faster? Would that have to be spurred by empirical data?

JK: everything needs to be supported by data. Unaware of specific plans to make things faster. Anything is fair game if we have data indicating it is a bottleneck. Too many possibilities to look at.

TL: is there low hanging fruit there? Or tough nut to crack?

JK: think it is a tough nut to crack. If we know fun call is high overhead we will look at inlining. Hard to do with vtables. No specific ideas of quick things we can do to improve call ref performance. If it turns out to be important, we will look at it.

ZB: if we want to support OO languages like Java… e.g. nullable refs could be reduced if we have a way to freeze object after creation. We need to have nullable fields during creation, during calling constructor, but after that we can freeze the object and after that some of the fields could be treated as non-nullable

TL: freezing after creation has come up on GitHub, seems like we should continue those discussions, seems common and important

RT: we have been talking about type canon. There might be an issue that canonicalization costs us various properties, was going to talk about encapsulation, because you are not implementing canon, you have encapsulation for free. When you implement that, you lose encapsulation. For debugging you have names, different rtts for the same struct within module or cross module, when you canonicalize that goes away. Metadata in rtt, canon doesn’t work with metadata. For JS interop, when you ship an object to JS land, has data for how to interpret that in JS. Canon will break all those use cases.

TL: a note on debugging, in C++ land that just having source maps or original variable names have been insufficient for having a good debugging experience. No matter what happens, we want to do a bunch of work on debugging, probably a whole bunch of metadata, something like DWARF, maybe reuse Wasm DWARF or something else. So much extra layer and information for debugging. Debugging considerations on low level semantics of language probably won’t matter that much.















