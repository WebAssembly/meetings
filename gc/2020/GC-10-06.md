![WebAssembly logo](/images/WebAssembly.png)

## Agenda for the September 22nd video call of WebAssembly's Garbage Collection Subgroup

- **Where**: zoom.us
- **When**: October 6th, 4pm-5pm UTC (October 6th, 9am-10am Pacific Daylight Time)
- **Location**: *link on calendar invite*

### Registration

None required if you've attended before. Fill out the form here to sign up if
it's your first time: https://forms.gle/JehrAB4gWbtHjybt9. The meeting is open
to CG members only.

## Logistics

The meeting will be on a zoom.us video conference.
Installation is required, see the calendar invite.

## Agenda items

1. Opening, welcome and roll call
    1. Opening of the meeting
    1. Introduction of attendees
1. Find volunteers for note taking (acting chair to volunteer)
1. Adoption of the agenda
1. Proposals and discussions
    1. Discussion: [Superfluous Casts](https://github.com/WebAssembly/gc/issues/120) (Ross Tate) [30 min]
    1. Discussion: Shared and Separate GC Heaps (Francis McCabe) [30 min]
1. Closure

## Meeting Notes

### Introduction of attendees

Thomas Lively
Ryan Hunt
Daniel Wirtz
Luke Wagner
Zalim Bashorov
Jakob Kummerow
Derek Schuff
Ross Tate
Manos Koukoutos
Wouter van Oortmerssen
Tim Steenvoorden
Rick Battagline
Conrad Watt
Benjamin Titzer
Keith Miller
Slava Kuzmich
Sabine
Lars Hansen
Yulia Startsev
Emanuel Ziegler
Adam Klein
Andreas Rossberg
Daniel Ehrenberg
Francis McCabe

### Discussion: [Superfluous Casts](https://github.com/WebAssembly/gc/issues/120) (Ross Tate) [30 min]

Slides: [(pdf)](presentations/2020-10-06-tate-superfluous-casts.pdf) [(pptx)](presentations/2020-10-06-tate-superfluous-casts.pptx)

RT: <Presentation> Goal is to get the group’s ideas on superfluous casts: casts that will never fail but that the generator cannot communicate because the type system is not expressive enough.

CW: question: Is the reason that MVP can’t guarantee safety that the source language has no variance on it’s array types?

RT: covariance makes it harder, but still a problem anyway, e.g. kotlin array. For Kotlin, let’s say they are invariant, only covariants if backend needs it. Kotlin array still and object, need some way to cast to kotlin array, on some backends they are reified. Need Foo array, get arguments and check it is a Foo array. None of the MVP can express that invariants. Even invariant arrays are not expressible, covariant ones are even harder to express.

CW: The covariant examples are already compelling, was just trying to understand more.

RT: at anypoint in Wasm type system, if it cannot reason enough about surface type, surface has to fallback to cast mechanism, then same problem with the casting mechanism to express that

CW: In abstract, there could be lots of reasons for that. But this seems particularly bad for langs with invariant arrays.

AR: can you explain the invariant problem again?

RT: Are you thinking about polymorphic or monomorphic system?

AR: monomorphic is fine.

RT: with a purely monomorphic system you can’t express many languages, then you need some way to deal with polymorphic system, to reason across arrays, arrays of T. If just monomphophic then just RTT for each type. Don’t think that will work for polymorphic setting.

CW: Fine to concentrate on source languages with covariant arrays. It seems that simple monomorphic array languages would map down ok.

??: generic library processing arrays, same problem

RT: Depends on whether it’s generic and reified or generic and not reified. The point is that there are systems where this is all possible to express, i.e. that can maintain these invariants. But not in our MVP.

DE: Is it a problem to have monomophic type system in Wasm, forcing you to insert casts when you have polymorphic types?

RT: if you take a monomorphic system… you have to prove polymorphic code respects monomorphic. Difficulty with invariance is that if you have one spot that breaks that, it’s broken everywhere. Sometimes you can restore that with a cast mechanism.

FM: If you focus on Java, you can’t have a reified implementation due to other things going on in the language.

RT: java arrays are reified, known pain point

CW: this is the way they escape the unsound, they carry runtime type of array.

RT: It’s not just escape covariance. If I have an array of objects and someone says it’s an array of Strings, I have to be able to check that. Need reification if you want the property that you can only put Strings into a String array.

FM: was thinking about specialization not reification.

RT: yea, related but different

FM: think you’re saying, even if you reify, there is an additional cost because of the MVP. java has its own cost, there is an additional cost.

RT: Yes, the benchmark show’s the cost due to the MVP. This happens with many generic data structures, not just arrays. Do people care about these overheads? Do we

TL: I would like to eliminate overheads, the meat in that discussion is, what is it going to take to make that happen.

CW: from my POV, it’s switch to nominal type system with bounded quantification.

RT: that’s how i know to get rid of it, another way is to find a way to extend current mvp, but I don’t know how to do that. Another question is do we want to get rid of it. Do we want it and what tradeoffs are we willing to consider.

FM: no one likes paying taxes

CW: 12% overhead on real code is extremely optimistic for java compiled to Wasm

FM: what do you mean optimistic?


CW: think it will be worse, especiall MVP

RT: purely cast, there will be other overhead

TL: any early estimates on what the overall overhead is attributable to superfluous cast? We have a more nuanced understanding once we have more experimental results from early partners.

FM: have to run the experiment to find out.

RT: can try to see if we can get someone to do these experiments. Have to build full MVP, then need generator to mark which casts are not necessary, need engine to emit those casts when running in a trusted setting

CW: you can do something dumber, delete all cast, put as switch in engine to not typecheck and generate code blindly. It wouldn’t be safe, only offline benchmark.

RT: still need type information for direct compilation, we need some mechanism to ignore casts, trust that they are true casts. Other issues are, when you break invariance in one spot, everywhere is broken. Say ML, other aspects of ML that mvp and post mvp can’t typecheck, same for Haskell, you have to reify the entire system, all polymorphism just to get invariants working. Generating a different implementation of these languages because you can’t typecheck them.

TL: question for JK, is this sort of tweak a thing and do a performance measurement something you’ll be interested in doing? Once we have full end-to-end prototype.

RT: can’t hear you, mic turn off and on

JK (chat): I can certainly try to get numbers like this. Shouldn't be too hard to hack the prototype to just not emit any machine code for casts

TL: on a good path to do an experiment, agree that will be useful

RT: about half an hour, FM mic working?

CW: quick question for AR, any idea for how MVP will accept these casts

AR: accepted overhead either way

CW: moving forward from mvp

AR: i can envision various ways to incorporate more advance type system features, honestly don’t understand how nominal type system will work here, might fix some problems but cause others. Don’t have a good answer

CW: for nominal, all the arrays getting a type X extend real type, with bounded quant you can push it everywhere.

RT: multiple papers showing how to do that, known how to do it.

AR: one where arrays are kind of primitive

RT: primitive in the sense of a bunch of values

AR: where arrays are structural, or arrays of generic type, whatever you cal lit. Unless we have generics, i don’t see how we can model arrays in nominal way.

CW: then we need generics

AR: then we need a whole new thing before we can get there

CW: that’s exactly the hypothetical bounded quantification extension

TL: continue diving into this on github

### Discussion: Shared and Separate GC Heaps (Francis McCabe) [30

[Slides](https://docs.google.com/presentation/d/13MaCHhGH6pP7jE10vCYJ2h7KR4qvDqfkAO0UdlfVEJg/edit?usp=sharing)

FM: <presentation on GC & threads> Explains requirement in requirements doc for having multiple GC memories.

CW: By not allowing JS to be threaded, do you mean concurrent access to JS values (as opposed to e.g. SharedArrayBuffer).

FM: we’re talking about giving access to JS objects here, if we are not careful about supporting multiple threads, it will allow JS to be multithreaded

TL: seems like we don’t need to design this for MVP in particular, whatever we do for MVP is forward compatible

FM: worth thinking about how one might solve the problem, without actually nailing everything down. Have roadmap so we don’t box ourselves in unnecessarily

CW: at least some people have though, we can mark table and refs as shared. Semantically what does that mean? But that’s how we support threads in java.

LW: To add to that, any shared thing can only point to other shared things. Sharedness has to be known at creation time. Single-threaded things can point to shared things, though. If you had a way to start another thread that had unshared things, that would imply that there is a new disjoint heap.

AR: in addition you want shared attribute on functions, this is all in our weak memory paper actually, in the appendix

WO: full multithreaded java program wants to communicate with JS, has to do it via copy or serialization

LW: I think there could be a notion of a “home thread” that would be able to call out to <I didn’t understand this>

CW: the idea would be to have two different web workers in JS, each has a ref to a shared table, instantiate Wasm modules to that table, sharing it, accessing it concurrently.

FM: Not talking about tables here, talking about memory with direct pointers.

LW: that’s an extension of tables

RT: java object that points to js object

CW: Transitivity property of sharedness works with JS objects always being unshared.

LW: if you had that restriction, then you have a non-shared table with all JS objects, stick an index to a shared java index. A home thread notion. Force upon us by the constraint FM brought up

TT: is it a constraint? Simpler model where we don’t allow non-shared heap, scavengers and minor gcs will be stop the world. Then you can allow several threads sharing objects, as long as you don’t access JS object except from main thread. If you try to access from off-thread, either stop the world or fails.

LW: shared things don’t point to unshared things is the main restriction, not the main thread. Big problem on main thread in the spec is you can’t use atomic.wait, although you can spinlock by accessing shared memory. We don’t allow blocking on main thread, limits what you can do on shared memory.

RT: I remember in the requirements discussion, there was a subdiscussion that Erlang wouldn’t be able to use GC for some reason?

FM: two scenarios, java shares everything in multiple threads, or multiple threads that don’t share. In erlang they want a super fast dispose that get rid of the entire process

LW: a spawn instruction, that doesn’t share with caller. WIth this spawn instruction, use totally unshared GC memory, each spawned thread has its own process, with green threading as the implementation, a cheap segmented stack, then that could be a reasonable basis to base erland spawn off of.

FM: we’re talking about… not just the stack, but also the heap you need to spawn off. What should we put in the requirements document. Thou shalt have threads

CW: makes sense to note it down. In my head it exists, but not surfaced in documents.

TL: possible that we can have threads without a segmented heap idea? Those seem intertwined, we discussed how impl of thread would use disjoint heaps, this disjoint heaps has been proposed as own feature independent of threads, e.g. for Erlang. Independent of threads, is disjoint heap important enough that it should get its own requirement?

RT: if you can have heap that are disjoint, but you know where the x-refs are, then becomes easier to collect cycles within heap space. Then do a larger occasional cross-space cycle thing.

FM: just to follow through, in the current world without GC. WIth GC, you have a worker thread, you have multiple memories, each module instantiated differently. We’re in danger of getting to, coalesce multiple memories into a single one, then the pressure on renderer becomes important. We don’t have multiple memories then pressure becomes and issue, and performance becomes an issue.

WO: I have argued that having multiple heaps for GC has many advantages and I wasn’t even thinking about threads. Shared references seems inherently problematic and should use interface types instead.

TT: in such a model you can’t do java right?

CW: we’ll need truly concurrent GC at some point due to the language we want to support

LW: These are not mutually exclusive. It’s desirable to have a shared GC heap and also to have isolated heaps.

CW: at the language level, when creating a new gc object, can it be a particular kind of symbol where all family of the same objects sit

LW: I think that will be hard. We’ve considered the symmetric problem in C/C++ world. And it’s not feasible to tag every pointer with its memory. Want to run different languages in different “processes” and have them communicate without sharing.

RT: region based heap and gc, in the surface language it’s too complicated for what most languages want, but when generating the code, it’s easier to imagine generating code to deal with regions.

CW: It is something that could be forwards compatible. If we want to add annotations

TL: in mvp wasm, single memory, then later expanded to multiple memories. Even in the beginning with single memory, that memory was declared in the module even with just 1, with limits, import and export. If we exactly mirror that design, then in the MVP we should have a GC heap to declare possibly with limits, but only 1 of them. In the future extend that to multiple GC heaps. Nothing like that has been proposed, though. Should we consider doing that?

FM: that’s a beginning, you’ll have to have a spawn instruction.

RT: TL is saying tha can happen later. Should struct.new or whatever instruction, should that have an immediat that must be 0, that refers to the index space of GC heaps?

RT: then you might need to bake them into the types, so you know which refs belong to which heaps

AR: what’s the exact benefit?

TL: if all the concepts are in place, and instructions have these immediates, then the future extension to support multiple gc heaps is trivial, relax those constraints.

AR: but what do you get if you can be explicit about multiple gc heaps? How would it be observable. There would be an instruction that make use of this?

TL: isolated gc heaps can’t reference each other, some shared some unshared

RT: instruction making a new struct in a given heap, the tyeps correspond to that heap

AR: all that is imposing restrictions, what do you get out of those restriction?

TL: theorized to have perf benefits

CW: concrete example is to throw away entire gc heap at once. Fast green thread

RT: … for free, shared heap

AR: don’t understand how throwing away heap, if you still have refs you can’t throw it away, it won’t be observable in anyway, maybe just be an optimization hint. Like a page in the larger heap, or a set of pages, maybe GC has some hint to evacuate them. SEmantically i don’t see what you get out of it.

TT: main benefit is separate collection

RT: maybe not a semantic thing, lets you maintain separation that are then useful for various performance things

BT: this is going in the direction of doing region analysis, problem is that we need to have region annotation everywhere, args to all functions, returns, you need to param function over region, a completely orthogonal dimension in type system that goes viral. Not sure if see benefit from this region inference, if you already have a GC.

TL: we are out of time, need to explore cost and benefit ideas more, including using it for threads.

JK (chat, and not in the right timeline): I think the Erlang memory management model doesn't _necessarily_ need separate memories. For instance, V8's GC cost is proportional to the number of live objects. So having a short-lived thread create a bunch of garbage and then die would be fairly efficient -- maybe slightly less efficient than per-thread memories (as native Erlang has), but not necessarily deal-breakingly so. (Which _isn't_ to say that Wasm _shouldn't_ have multiple disjoint memories.)
