![WebAssembly logo](/images/WebAssembly.png)

## Agenda for the July 26 video call of WebAssembly's Garbage Collection Subgroup

- **Where**: zoom.us
- **When**: July 26, 5pm-6pm UTC (July 26, 9am-11am Pacific Time)
- **Location**: *link on calendar invite*

### Registration

None required if you've attended before. Fill out the form here to sign up if
it's your first time: https://forms.gle/JehrAB4gWbtHjybt9. The meeting is open
to CG members only.

## Logistics

The meeting will be on a zoom.us video conference.
Installation is required, see the calendar invite.

## Agenda items

1. Opening, welcome and roll call
    1. Opening of the meeting
    1. Introduction of attendees
1. Find volunteers for note taking (acting chair to volunteer)
1. Adoption of the agenda
1. Proposals and discussions
    1. Discussion: usefulness of a `wasm` type (Ben Titzer, 30 minutes)
    2. Discussion: type annotations on accessor instructions
1. Closure

## Meeting Notes

### Introduction of attendees

- Thomas Lively
- Sergey Rubanov
- Aske Simon Christensen
- Rick Battagline
- Justin Michaud
- Asumu Takikawa
- Ben Titzer
- Conrad Watt
- Yulia Startsev
- Andreas Rossberg
- Ilya Rezvov
- Francis McCabe
- Adam Klein
- Alon Zakai
- Deepti Gandluri
- Manos Koukoutos
- Luke Wagner
- Jakob Kummerow
- Emanuel Ziegler
- Zalim Bashorov
- Matthias Liedtke
 






### Discussion: usefulness of a `wasm` type (Ben Titzer, 30 minutes)

BT: Introduce a supertype of all the Wasm things that excludes internalized external things. But given that we decided on a three-prong hierarchy, I think data already serves this purpose.

CW: We should have a wider discussion about how wide we want casts to be.

BT: The principle I was after was to give modules the ability to be specific about types they don’t want to include. To allow negative reasoning.

TL: The casting discussion is one we have a start to, but I expect that we’ll be actively discussing this soon


### Discussion: type annotations on accessor instructions

TL: I think we shouldn’t have type annotations because the size savings seemed significant.

CW: We should look at relaxed dead code validation
TL: How much work would it be? It’s a NOP in binaryen

CW: We came to the conclusion that we shouldn’t have a distinbction between decode/validation errors. We have only one path forward, and does that implementation effect validation performance?

CW: <TODO: Link proposal for context> When we only have a bottom stack type, in terms of validation rules does this degrade validation performance? 

BT: My 2c. All the checks for validation in Wizard look like “if we’re in unreachable code, don’t do X.” This is easier than actually tracking a bottom type.

CW: That’s a way to do it.

TL: Don’t want the GC proposal blocked on the dead code validation proposal, but if it is simple, we should get it through, we have several issues to figure out in the GC world, we could explore advancing this in parallel

AR: Depending on how the implementation works, it might make it more complicated because you have more case distinction, makes the spec complicated as well. As it sits right now, you would have a whole set of adhoc rules added to the spec. Wr.r.t implementations you’d have to figure out whether it makes their lives simpler, depends on how they work. If you’re using the right implementation, the current semantics may be easier to handle. Hard to predict

TL: If we decide not to have type annotations, from the spec side we end up having adhoc rules anyway, would it be worse to have explicit rules from the dead code validation? Is one worse than the other?

AR: There would be fewer ad hoc rules.

CW: It would be similar. In worst case, you have two separate rules: one for reachable code and one for unreachable code.

TL: On the issue, JK said we should make this decision based on reachable code, and not unreachable code. I agree with that, but this is good background for what needs to happen in the spec

BT: Tools that process code cannot understand what the stack effects are unless they also model types, there’s different manifestation in different tiers, interpreters, baseline compilers fall into the same category where they don’t have types rolled into it. Not all baseline compilers are going to be using types, you need atleast ⅔ instructions to model types. These two tiers are stand ins for tools that process bytecodes, they will not have to replicate the extra work for types, which means they’ll be doing work they don’t need to, we should be cautious about what we decide to do. 

AR: Was at the first program analysis workshop for Wasm a few weeks ago. Don’t know if these particular problems would be problematic for them, but explicitness was definitely a selling point.

TL: What type of tooling doesn’t need to reason about types? In the context of a whole tool I assume they need to do something with types. Without an additional concrete use cases, not willing to put much stock into..

AR: They already exist, it’s just much harder for existing tools. 

LW: We’ve invested a lot of time into linear type validation, so we could lean into that. It is cheap and easy to do, both V8, and spider monkey have factored that into a templatized iterator

JK: Once you do have a wasm-aware tool, it is really easy to recover the types. If writing an analyzer from scratch, I can see that it would be more work to track types, but in practice it would be easy for existing disassemblers to emit types as well.

TL: Binaryen already has a mode where we print types alongside code.

BT: We can’t really know what all consumers look like, we’ve talked about streaming decoders, people will look at less work, avoiding overloading is one thing we should do

TL: How do you think it stacks up against the size savings that we were seeing? 5-6%

BT: Those were the compressed savings, it’s closer to 2-3% uncompressed, different value systems, personally don’t care that much about the 5% savings

AK: It seems weird to me to focus on researchers rather than Web users. Libraries are an easy solution for the research need, but at the scale of the Web that 5% savings becomes very large.

BT: I understand the priority, there are other things that we should do that get bigger wins

AR: I’m concerned by code size discussions, we had long discussions about this early on, the producer can design their own bytecode if needed, we decided that this was not immediately obvious as a thing to do, if needed, we could introduce a MACRO language or a compression scheme for example that would have bigger size savings instead of micro optimizations that don’t result in much

TL: We should investigate those options, but I’m not for adding extra bytes when the 2-3% savings are significant

JM: Implementers are already concerned about numbers much smaller than 5%

AR: We’re talking about space, or not time

JM: Not familiar with Layer 1 compression, what are the comparative savings? 

TL: In the presence of a Wasm specific compression scheme, these extra bytes wouldn’t matter at all because it would remove them altogether

BT: Should we just move it out of the GC space into the main space and just save the byte? 

JK: We could do that and I posted some numbers from that experiment. I found that removing the type index is more impactful than removing GC prefixes under brotli. The type annotations have sufficient entropy that they are hard to compress, unlike replacing fixed two-byte sequences with a one-byte sequence.

JK presenting [data](#issuecomment-1194749304)

BT: It feels like hard for us to optimize against a specific compression algorithm, we can measure these things, but how do we argue that this specific compression algorithm works in a certain way, and make some design decisions based on these findings.

JK: I don’t think we need to be so specific. You can also look at gzip or some other compressor.

BT: We can do it at any point of time in the future, we don’t have to do it now

AR: We should keep the overall type system in mind, example with select, I don’t see a particular issue, but I’m not for trading the robustness for the overall type system for a 2% size saving

JK: The 2% on this module will get bigger as we take advantage of other opportunities for size 
savings. Aske reported 5-6% on the barista3 dart benchmark.

AS: Only looked at the code section, I was seeing these as 5.7 uncompressed ~6 uncompressed, that’s an interesting number, you could have a module with ⅔ of the module being data and we wouldn’t save anything

TL: How do we make progress? We can go either way. Different enough opinions here that we won’t get unanimous consensus one way or the other. We could do one of the polls on a github issue to measure the temperature and take it from there

LW: Could validation time data help make a decision here? 

BT: Since this is a space optimization, it could fit into an optimization phase, there could be more wins overall instead of making a one off design decision on this case

CW: Validation time differences would make that a weaker argument.

LW: Also will need to do additional validation checks

AR: We don’t really know how much that affects overall compilation time, would be interesting to know. Have changed my position on this a couple of times, now deciding to err on the side of maintaining the robustness of the type systems because type systems are definitely brittle, can’t really prove it in any way, 

TL: The select fix of adding a new instruction with annotation is pretty simple in practice.

AR: Hard to say whether it would be simple in a different situation. We don’t know what would break, so it’s hard to say what the fix would look like.

DG: What size wins would be large enough now?

AZ: We’re aware of code size issues, there’s a very large overhead of string overhead, I would expect these size savings to be significantly larger depending on how optimized the binary is

CW: Are these numbers we could get by just building with a different flag?

AZ: We can test it, but it’s not optimized very much, we could potentially wait a bit and get better numbers, it’ll be significantly more, can’t quantify it yet, but expect it to be significant

JM: Even if it is only 5% or 2 %, we would expect users to download that extra overhead for type system robustness, but if it’s only for tooling that’s not probably worth it

TL: It’s a guard against future breakage that may or may not happen, 

CW: Some thing polymorphic gets on the type stack, maybe we accidentally let implicit typing into the languages as examples of concrete breakage. 

AR: We can’t add something in the language because of implicit typing

LW: Incidentally, concerned about adding all the validation rules fior dead code validation, but maybe relaxed dead code validation might help

CW: We keep hitting up against needing relaxed dead code validation in different proposals

BT: With the local initialization, 1a is based on some inference. Types from the stack can be more precise than the annotations, so for example nullability can change. Annotations are a nice “check” to make sure everyone is looking at the same type rather than depending on having the right type from the stack.

MK: What if we add the instruction, but add type annotations to other instructions, in the cases where there is type confusion, you can annotate those with types

CW: If we get into the bad situation of needing annotations later, you would need to figure out how to add them, so from the wasm perspective it’s the same as any other instruction

MK: I would assume this would be a rare fallback.

AR: Problem is not future features, but future instructions that conflict with existing instructions, you can add annotations then, but they’re optional and you would still handle the case when they’re not there

MK: I guess it would be a validation error if you couldn’t infer the type.

CW: You would have to find some way to make the distinction, in the wors case, allk these problems are solvable, but it’s just avoiding a pit to fall into

AR: You can’t randomly rule out some types without breaking some properties like substitutability. and some positions, don’t think this problem is that hard to avoid. Hard to predict, but also very fragile

MK: Point of the annotations is that engine doesn’t need to do inference

AR: Problem is defining the problem when you don’t have annotations, and now when you don’t have them you have to do it in a way that breaks the type system, it’s not obvious that that’s possible

BT: We talked about this very early on, e.g. can you use the expected return type of an AST node to compress the next opcode? We could go deep down this road.

AR: similarly with overloading, you just add it to one instruction, I’m sure that compression will be beneficial overall, this is really a type of overloading

TL: Given that you have a concrete type on top of your stack, but that doesn’t need an annotation because you can get it from the top of your stack, if we end up in a situation where you don’t know the type of your stack, and can’t infer it..

BT: You can get into a lot of weirdness with generics, not comfortable with this line of reasoning

TL: If that means we need to add type annotations to a future proposal, so be it

AR: It could maneuver you into a position where you might not be able to add them

TL: Comes down to value systems, there’s some risk of breakage, the solution of adding a whole slew of annotated instructions even though ugly it’s solvable. Good next step is to do a quick poll and get a better sense of where everyone stands. 
ASC: Adding annotated instructions doesn’t sound so bad in my opinion. We have different places where we have special cases of instructions

AK: Is there a blocker to get validation time numbers? Binary size isn’t the thing I care most about for web users

TL: Would anyone individually change their opinion if there was improvements in validation time? 

CW: On the fence, but if the validation time is bad, then I would vote for removing

AR: We should look at validation as a % of compile time, it would have to be significant for me to change my mind, more than 2% definitely

TL: Might be useful to get this data

JK: It’s doable.
