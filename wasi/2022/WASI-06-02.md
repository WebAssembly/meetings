![WASI logo](https://raw.githubusercontent.com/WebAssembly/WASI/main/WASI.png)

## Agenda: June 2 WASI video call

- **Where**: zoom.us (see Registration below)
- **When**: June 2, 16:00-17:00 UTC
- **Contact**:
  - Name: Lin Clark
  - Email: lclark@fastly.com

### Registration

If this is your first time attending, please [fill out the registration form](https://docs.google.com/forms/d/e/1FAIpQLSdpO6Lp2L_dZ2_oiDgzjKx7pb7s2YYHjeSIyfHWZZGSKoZKWQ/viewform?usp=sf_link) to receive an invite.

The meeting is open to CG members only. You can [join the CG here](https://www.w3.org/community/webassembly/).

## Logistics

The meeting will be on a zoom.us video conference.

## Agenda items

1. Opening, welcome and roll call
    1. Please help add your name to the meeting notes.
    1. Please help take notes.
    1. Thanks!
1. Announcements
    1. _Sumbit a PR to add your announcement here_
1. Proposals and discussions
    1. Component Model async proposal (updated) (slides: [pdf](presentations/2022-06-02-luke-async.pdf), [live](https://docs.google.com/presentation/d/1MNVOZ8hdofO3tI0szg_i-Yoy0N2QPU2C--LzVuoGSlE))

## Notes

### Attendees
- Lin Clark
- George Kulakowski
- Andrew Brown
- Jeff Charles
- Bailey Hayes
- Saúl Cabrera
- Luke Wagner
- Adam Mohammed
- Dan Gohman
- Yong He
- Dave Bakker
- Till Schneidereit
- Alex Crichton
- Mingqiu Sun
- Sebastien Deleuze
- Josh Triplett
- Aaron Schlesinger
- danbugs
- Jeff Charles
- Johnnie Birch
- Mossaka
- Petr Penzin
- Piotr Sikora
- Ralph Squillace
- Syrus Akbary
- Thomas Lively
- Granville Schmidt
- Kevin Moore

### Component Model async proposal (updated)

[live slides on the component model](https://docs.google.com/presentation/d/1MNVOZ8hdofO3tI0szg_i-Yoy0N2QPU2C--LzVuoGSlE/)

**Lin Clark**: Part 1 of 2 on this proposal, which Luke expects will take more than 1 meeting

**Luke Wagner**: update from last time
- motivation
- background: synchronous Canonical ABI
- Async support
  - future
  - callback abi (optimization)
  - eager return (optimization)
  - stream (optimization)
  - splicing and skipping stream (optimization)
- more for next time :)

- motivation
  - How do we specify async/non-blocking operations in WASI and wit?
  - Can’t we just add first-class functions / callbacks to wit?
    - Cycle leak problems especially in non-GC settings
      - note that browsers are able to de this in js settings by controlling code on both sides
    - Callbacks can be very low-level, especially compared to language concurrency models
  - reqs
    - virtualizability
    - efficient i/o when interfacing with the host (epoll, io_uring)
    - ergonomic automatic wit-bindgen bindings
    - support different styles of lang-level concurrency
      - sync
      - nonblocking
      - async
      - coroutine
    - built-in backpressure story
    - integrated select/timeout/cancellation
    - keep executing after returning a final value
    - not force threads, but allow them
- background: synchronous canonical ABI
  - associating core wasm types to component types named in imports/exports
  - canon lift / lower operations
  - produce a core wasm file
  - new, we can wrap this up into a component
    - .wasm + .wit + ABI options
    - now via wit-component
    - maybe later via ldd or equivalent
  - canon abi options
    - example, string encoding to describe and match what you have internally
  - canon lift and lower bracket all component entry/exit operations
  - example of different string encoding options in rust (utf8) vs js (latin1+utf16) being linked together
    - compiler can see all of these together and optimize
      - eg the cross-linear-memory string copy can be fused with and optimized the string transcoding
  - in separate processes, so not like classic async ipc writing into a pipe
- new stuff for async now in wit
  - future type constructor
  - future<T>, eg future<String>
  - still canon lift/lowered
  - also canon return
    - non blocking
    - offering a T for the given future
    - the ptr to the offered T must stay valid until the return-complete event
  - canon wait
    - block until an event; events include
      - return-complete
      - return
  - canon listen
    - offer a buffer to receive the future’s value
    - buffer must stay valid until the return event
    - traps if futures come from canon lower
  - futures can have very different roles
  - in particular coming from imports or exports
  - eg trapping on wrong canon listens
  - can write idiomatic code that says e.g. async/await and have the toolchain lower to the canonical abi
- question time

**Andrew Brown**
- <audio quality was a bit rough, question around futures given vs returned>
**Luke Wagner**
- some are more like out params

**Josh Triplett**
- why allow running code after offering a value?
**Luke Wagner**
- well, streams will generalize this.
- but also something like http handlers. maybe you send out the request response per se, but you still want to do some logging that’s associated with the request but not in the hot path

**Petr Penzin**
- you talk about lowering+lifting, and so those choices mean more for what it means to instantiate the module
**Luke Wagner**
- yes, and in particular with linking we want to have fusing of those adapters of both sides.
- we might want to put this into the spec and describe the “link” step, which is a good time for AOT compilation.
- note also that in the case with one core module, the host could just implement all the imports!
**Petr Penzin**
- what if we use components and a link step, doesn’t that imply having all dependencies statically?
**Luke Wagner**
- loading code is a subtle thing. i expect implementations to be able to load lazily or not. so we want to be able to AOT compile, but implementations may make different choices.

**Josh Triplett**
- listen and return are both nonblocking. so it seems like there’s not enough information to fuse writes by seeing that the buffer is already there
**Luke Wagner**
- “eager return” is the very first optimization we’re going to talk about
**Josh Triplett**
- what about with streams and multiple values?
**Luke Wagner**
- scatter/gather, nope not yet. worth considering. we can do N at once now though, but lets cover streams.

**Luke Wagner**
- back to presentation
- linked components, working through the control flow
  - something like fetch is asynchronous, so control is yielded back with a returned future
  - the calling code could eg start more async work
  - nthen the calling code waits, control yielded back to its caller, which in this case is across the component boundary
  - eventually the IO completes and the host task gets to copy that data to its caller-provided buffer and so on
  - very chatty without optimization, hence things like eager-return

**George Kulakowski**
- What’s backpressure going to look like?
Luke Wagner
- going to talk more later, but looks like tasks you have but not listening

**Luke Wagner**
- unsure if the futures should be i32 or i64 in wasm64, as they are indices into tables

**Luke Wagner**
- back to presentation
- optimization: callback ABI
  - consider future/promise/task async+await languages
    - rust .NET JS ...
    - wait is always performed at the base of the callstack
    - want to allow producer toolchains to leverage that constraint and elide the stack switch
  - some hosts, native stack switching doesn’t exist or is expensive
  - so generally want to allow callback ABI to directly call
    - and a bool of whether you are finished
  - encapsulated implementation detail
  - composes with non-callback
  - directly call rather than suspend stacks
  - composable, can use both

**Bailey Hayes**
- do you think you can eliminate some of these copies?
**Luke Wagner**
- some of these copies are ones that are semantically copies out of the host. that could be eg via io_uring to go directly into the userspace buffer.
- some are what we conceded with the shared-nothing component model.

**Petr Penzin**
- using the callback ABI means compiling the code very differently?
**Luke Wagner**
- yes! I suspect toolchains will do all one or the other, generally. can imagine wanting to expose this more fine-grainedly in some contexts
**Petr Penzin**
- yeah, openmp say
**Luke Wagner**
- one thing we should be able to do is implement existing libc in terms of this. kick of, listen, wait
- stack switching proposal would allow blocking ABI to run on the web context. specifically, the promise integration. asyncify would also work but expensive

**Josh Triplett**
- what is the story for cancellation?
**Luke Wagner**
- part 2 structured concurrency will address that

**Kevin Moore**
- hi from the dart team. do we think we can maintain our individual quirks in terms of runtime semantics?
**Luke Wagner**
- we do. one way we think of this is that languages already bridge this by running on different OS semantics. and so we think this is general and flexible enough
**Kevin Moore**
- what about debugging? eg stack traces
**Luke Wagner**
- also part of structured concurrency. tasks forming a tree helps.
- to be fair, cross language call stacks challenging. the language will need to help the debugger traverse each language’s part of the call stack

**Petr Penzin**
- following on with Kevin and wondering about what the different semantics are, and what dart has
**Kevin Moore**
- i’d have to talk to our js engineers. know there are some subtle differences, like when tasks start.
- want to see if all the different models
**Luke Wagner**
- being able to compile to JS is a good sign that you are adaptable
**Petr Penzin**
- depends how sophisticated your state machine is perhaps

**Luke Wagner**
- back to presentation
- optimization: eager return
  - if the result is already available, future adds overhead
  - add nuance to the async cabi operations to know if they are done etc
  - brings us very close to a synchronous looking callstack

**Bailey Hayes**
- are implementations already in flight?
**Luke Wagner**
- maybe Alex can speak more to what he’s worked on
**Alex Crichton**
- we’re working on component model work but not yet to the async work
**Bailey Hayes**
- a lot feels similar to prototypes of async work that Alex showed
**Alex Crichton**
- ah yes. a lot of this exists in a prototype state, in particular the compiling-down parts. but not a lot yet of async support in wasmtime yet.
