![WASI logo](https://raw.githubusercontent.com/WebAssembly/WASI/main/WASI.png)

## Agenda: March 24 WASI video call

- **Where**: zoom.us (see Registration below)
- **When**: March 24, 16:00-17:00 UTC
- **Contact**:
  - Name: Lin Clark
  - Email: lclark@fastly.com

### Registration

If this is your first time attending, please [fill out the registration form](https://docs.google.com/forms/d/e/1FAIpQLSdpO6Lp2L_dZ2_oiDgzjKx7pb7s2YYHjeSIyfHWZZGSKoZKWQ/viewform?usp=sf_link) to receive an invite.

The meeting is open to CG members only. You can [join the CG here](https://www.w3.org/community/webassembly/).

## Logistics

The meeting will be on a zoom.us video conference.

## Agenda items

1. Opening, welcome and roll call
    1. Please help add your name to the meeting notes.
    1. Please help take notes.
    1. Thanks!
1. Announcements
    1. _Sumbit a PR to add your announcement here_
1. Proposals and discussions
    1. An update on asynchrony in the component model, @aturon and @lukewagner 
    1. _Sumbit a PR to add your agenda item here_

## Notes

### Attendees

- Lin Clark
- Dave Bakker
- Saúl Cabrera
- Jeff Charles
- Sébastien Deleuze
- Dan Gohman
- George Kulakowski
- Piotr Sikora
- Mingqiu Sun
- Aaron Turon
- Radu Matei
- Yong He
- Nick Fitzgerald
- Bailey Hayes
- Till Schneidereit
- Andrew Brown
- David Justice
- Luke Wagner
- Jonnie Birch
- Peter Huene
- Brian Hardock
- Pat Hickey
- David Piepgrass
- Arne Vogel

### Async Update

**Aaron Turon:** Background, Alex and I worked together to develop Rust futures and async blocks in Rust, and as you’ll see many of the ideas apply nicely to the component model. Presentation is one that I gave internally, so high level. Hope is to give you an overall picture of where we’ve come in thinking about async in the component model.

Everything intended to fit into component model built on top of wasm standard. Each component should be written in idiomatic source language but collaborate with other components in diff langs.

Everything I’ll be presenting today will be in terms of canonical ABI, but ideas about how this will interact with adapters. There’s canonical ABI for stitching together, but translates between idiomatic structures and canonical ABI.

As part of this, building on top of Wasm execution model. In that model right now, you can set up a component graph with independent memory for each component and execution is driven by asking engine to execute top level export—the entry point. And exec is sync. At any time, one function in whole component graph. One interesting nuance here is that there’s a distinction between imports from host which can have extra capabilities. Goal is to keep gap between host and component capabilities as small as possible.

Kick off whole thing by execing top level export, which can call into another component or host. That all happens single threaded.

Explain the goals for async. Why async? Couple of distinct motivations. Expressiveness, ability to do things like launch concurrent ops and race them. Need it for idiomacy as well.

Have been keeping in mind different languages that represent the different models of async we want to support (futures, promises, goroutines). Avoiding gotchas for all these. Also make things efficient, and try to make as minimal a proposal as possible. Introducing concurrency risks losing properties, so we keep a single threaded underlying model of execution.

We are setting aside fully multi-threaded because of above, so we’re introducing tasks. It’s concurrent but not necessarily parallel.

We are going to have a notion of an async function import or export in IT. When you import an async function, you’re actually spawning a new task. To begin with, don’t expect to have a persistent stack. Not a greenthreaded kind of model. But we want to anticipate that greenthreads or coroutines could be added later. Way to think of a task is the inner body of an event loop where the el is being managed by the runtime. This is a fairly common impl approach even if langs don’t surface it directly. Internally, you can have lots of async tasks, …..

Final note, there are langs like Go that bake in greenthreaded. In meantime, use things like LLVM or asyncify to handle.

**Andrew Brown:** What about when you have async calls inside a component.

**Aaron Turon:** Depends on how you’ve set up language glue. We don’t expect that to be visible at the component model level. That’s internal to the component. I was mentioned having fine grained task when you cross boundary, but inside much coarser grained.

Every component in this world… the glue code involves a top level event loop. You can create internal concurrency within that component however you like. How you multiplex between tasks in your own event loop is up to you.

Alright.

So let’s get into meaty detail. [Async imports (simplified) slide walk through]

Any function that returns future result is async and will have semantics we’re describing.

**Andrew Brown:** example?

**Aaron Turon:** Example of request processing. Race different origins. You have an async export, and you might fork out to do requests to different origin servers. The way that’s going to look is to import send_request. You’ll call it twice, spawning two tasks. Let’s say send_request is host. What the host import is going to do is use its own async primitives. When it completes, it will notify you. While those backend requests are being processed, nothing in Wasm is running at all. That component calls wait to show that it’s blocked on child tasks. When one response comes back, we can go back into Wasm execution. We’re going to call onevent and say child return. I do how much compution I can do with just that event, and call wait again until the other comes back.

**Andrew Brown:** Helpful. Who calls listen function. 

**Aaron Turon:** Importer. It’s a blocking call but it ….

**Andrew Brown:** So that’s saying I do want to hear about events in these async tasks

**Aaron Turon:** And where to put the results of them

This lets us have single set of intrinsics that can work with arbitrary types. 

**Dave Bakker:** I’m familiar with future type of Rust. Holder of future value does the polling and progresses the task. If I understand, this model is opposite.

**Aaron Turon:** That’s exactly right. Rust has notion of tasks as well. Here task execution drives itself. We’re going to talk about cancelation later.

Next piece is talking about structured concurrency, so cancelation and post return. I introduced task model by saying everytime we invoke import, tantamount to spawning a task. Spawning has tricky edge cases like lifetime management. In lots of langs, not much structure. Top level API that you call to spawn fresh task and the relationship between spawner and spawnee is up in the air. Hard to guarantee that you’ve shut down a qhole task subgraph, hard to do and have to do in app code.

In recent years, has been push to structured model that give you more control and predictability around lifetimes of tasks.

Core idea is that we keep track of a task tree. A lot of ways to design structured concurrency. Some go to far in making it so every function needs to think of tasks. Our goal is to have clear lifetimes and reliably push out cancellation. Also allow for post-processing after return. Internally, it looks like new tasks that were spawned, but for you you don’t really care. We’re trying to balance between simple programming model, where you don’t have to think about lifetimes, but give you the option to manage those with things like cancelation if you want to.

**Dan Gohman:** So you might say that each lang is it’s own async runtime. 

**Aaron Turon:** thats actually great thing to bring up. Go does epoll to multiplex out to greenthreads. Each component invocation, you have a small runtime going on in the component managing the external events and then doing whatever makes sense internally. 

So I’ve been describing all of this in terms of component model and canonical abi. But we need to hook this into the programming language experience.

Goal is for developers to not have to think about this at all. Work idiomatically in their own language.

Every lang has a way to manage async requests to the system. We’re just another system. For Rust, we’ve worked through the glue level. We have something that roughly gives you async imports and exports that map to language-native async. Bottom line, you can just treat it as a normal future, not caring at all that there might be some spawned subtasks. 

Last piece which was really important and thorny is streams. Everything we’ve been talking about so far is single value interaction. Another really important use case is streaming async. Another type constructor like future. You don’t just get a pipe that you can pass around freely. The pipe is created at the same time as the task that’s job is to fill that pipe. Having this property was super helpful for things like preventing deadlocks.

Barely scratched the surface. A lot of fine detail that we’ve bottomed out in. Have an early prototype in Wasmtime and toolchain glue to check end programmer experience.

**Jonnie Birch:** What’s best place to follow progress here.

**Luke Wagner:** I think the next step is turning this writeup into PR to the component model. I’ll be working on that PR. THat will be pretty low level. Before that, I’ll be working on presentations to this group and the stack switching group. So that will come in the future weeks.

**Jonnie Birch:** Sounds good, lots to digest here.

**Sébastien Deleuze:** I may try to contact the Kotlin team because they also have structured concurrency.

**Luke Wagner:** One other thing, if someone is raring to go. This isn’t a good intro, so not useful as a starting point, but here’s the hack.md file:  https://hackmd.io/NM8WrCMWRPOwq--K1jRQrg?view

